# Pydantic AI æ¨¡å‹æ”¯æŒè¯¦è§£

> æœ¬æ–‡æ¡£ä»‹ç» Pydantic AI æ”¯æŒçš„æ‰€æœ‰æ¨¡å‹æä¾›å•†ã€é…ç½®æ–¹æ³•å’Œè‡ªå®šä¹‰æ¨¡å‹å®ç°ã€‚

## ğŸŒ æ”¯æŒçš„æ¨¡å‹æä¾›å•†

### å†…ç½®æ”¯æŒ

Pydantic AI åŸç”Ÿæ”¯æŒä»¥ä¸‹æä¾›å•†ï¼š

| æä¾›å•† | æ¨¡å‹ç¤ºä¾‹ | æ ¼å¼ |
|--------|---------|------|
| OpenAI | GPT-4, GPT-3.5 | `openai:gpt-4` |
| Anthropic | Claude 3.5 | `anthropic:claude-3-5-sonnet-20241022` |
| Google | Gemini Pro/Flash | `google:gemini-1.5-pro` |
| Groq | Llama 3, Mixtral | `groq:llama3-70b-8192` |
| Mistral | Mistral Large | `mistral:mistral-large-latest` |
| Cohere | Command R | `cohere:command-r-plus` |
| Cerebras | Llama 3.1 | `cerebras:llama3.1-8b` |
| HuggingFace | å„ç§å¼€æºæ¨¡å‹ | `huggingface:model-id` |
| Ollama | æœ¬åœ°æ¨¡å‹ | `ollama:llama3` |

### OpenAI å…¼å®¹æä¾›å•†

ä»¥ä¸‹æä¾›å•†ä½¿ç”¨ OpenAI å…¼å®¹çš„ API,å¯ä»¥é€šè¿‡ `OpenAIChatModel` ä½¿ç”¨ï¼š

- Alibaba Cloud (DashScope)
- Azure AI Foundry
- DeepSeek
- Fireworks AI
- GitHub Models
- Grok (xAI)
- Heroku
- LiteLLM
- Nebius AI Studio
- OVHcloud AI Endpoints
- Perplexity
- Together AI
- Vercel AI Gateway

## ğŸš€ å¿«é€Ÿå¼€å§‹

### OpenAI

```python
from pydantic_ai import Agent
import os

# è®¾ç½® API å¯†é’¥
os.environ['OPENAI_API_KEY'] = 'sk-...'

# ä½¿ç”¨é»˜è®¤æ¨¡å‹
agent = Agent('openai:gpt-4')

# ä½¿ç”¨å…¶ä»– OpenAI æ¨¡å‹
agent = Agent('openai:gpt-4-turbo')
agent = Agent('openai:gpt-3.5-turbo')
agent = Agent('openai:gpt-4o')  # GPT-4 Omni
```

### Anthropic

```python
import os

os.environ['ANTHROPIC_API_KEY'] = 'sk-ant-...'

# Claude 3.5 Sonnetï¼ˆæ¨èï¼‰
agent = Agent('anthropic:claude-3-5-sonnet-20241022')

# Claude 3 Opus
agent = Agent('anthropic:claude-3-opus-20240229')

# Claude 3 Haikuï¼ˆå¿«é€Ÿä¸”ä¾¿å®œï¼‰
agent = Agent('anthropic:claude-3-haiku-20240307')
```

### Google Gemini

```python
import os

os.environ['GOOGLE_API_KEY'] = '...'

# Gemini Pro
agent = Agent('google:gemini-1.5-pro')

# Gemini Flashï¼ˆå¿«é€Ÿï¼‰
agent = Agent('google:gemini-1.5-flash')
```

### Groqï¼ˆå…è´¹ä¸”å¿«é€Ÿï¼‰

```python
import os

os.environ['GROQ_API_KEY'] = 'gsk_...'

# Llama 3
agent = Agent('groq:llama3-70b-8192')
agent = Agent('groq:llama3-8b-8192')

# Mixtral
agent = Agent('groq:mixtral-8x7b-32768')

# Gemma
agent = Agent('groq:gemma-7b-it')
```

### Ollamaï¼ˆæœ¬åœ°è¿è¡Œï¼‰

```python
# æ— éœ€ API å¯†é’¥
agent = Agent('ollama:llama3')
agent = Agent('ollama:mistral')
agent = Agent('ollama:codellama')

# è‡ªå®šä¹‰ Ollama æœåŠ¡å™¨
from pydantic_ai.models.ollama import OllamaModel

model = OllamaModel(
    model_name='llama3',
    base_url='http://localhost:11434'
)
agent = Agent(model)
```

## âš™ï¸ æ¨¡å‹é…ç½®

### ModelSettings

```python
from pydantic_ai import Agent, ModelSettings

agent = Agent(
    'openai:gpt-4',
    model_settings=ModelSettings(
        temperature=0.7,           # åˆ›é€ æ€§ (0-2)
        max_tokens=2000,           # æœ€å¤§ç”Ÿæˆé•¿åº¦
        timeout=60,                # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        top_p=0.9,                # æ ¸é‡‡æ ·
        frequency_penalty=0.5,     # é¢‘ç‡æƒ©ç½š
        presence_penalty=0.5,      # å­˜åœ¨æƒ©ç½š
        stop=['END', '\n\n'],     # åœæ­¢åºåˆ—
    )
)
```

### é…ç½®ä¼˜å…ˆçº§

```python
# 1. æ¨¡å‹é»˜è®¤å€¼ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰
# ç”±æ¨¡å‹æä¾›å•†å®šä¹‰

# 2. Agent çº§åˆ«
agent = Agent(
    'openai:gpt-4',
    model_settings=ModelSettings(temperature=0.5)
)

# 3. è¿è¡Œæ—¶è¦†ç›–ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
result = agent.run_sync(
    'æç¤º',
    model_settings=ModelSettings(temperature=0.9)
)
```

### é’ˆå¯¹ä¸åŒæ¨¡å‹çš„é…ç½®

```python
# OpenAI ç‰¹å®šé…ç½®
openai_settings = ModelSettings(
    temperature=0.7,
    max_tokens=2000,
    response_format={'type': 'json_object'}  # JSON æ¨¡å¼
)

# Anthropic ç‰¹å®šé…ç½®
anthropic_settings = ModelSettings(
    temperature=0.5,
    max_tokens=4096,
    top_k=40  # Anthropic æ”¯æŒ top_k
)
```

## ğŸ”§ è‡ªå®šä¹‰æ¨¡å‹

### ä½¿ç”¨ OpenAI å…¼å®¹ API

```python
from pydantic_ai.models.openai import OpenAIChatModel

# DeepSeek ç¤ºä¾‹
deepseek_model = OpenAIChatModel(
    model_name='deepseek-chat',
    base_url='https://api.deepseek.com',
    api_key=os.getenv('DEEPSEEK_API_KEY')
)

agent = Agent(deepseek_model)

# Grok ç¤ºä¾‹
grok_model = OpenAIChatModel(
    model_name='grok-beta',
    base_url='https://api.x.ai/v1',
    api_key=os.getenv('XAI_API_KEY')
)

agent = Agent(grok_model)
```

### å®ç°è‡ªå®šä¹‰æ¨¡å‹

```python
from pydantic_ai import Model
from pydantic_ai.messages import (
    ModelMessage,
    ModelRequest,
    ModelResponse
)


class CustomModel(Model):
    """è‡ªå®šä¹‰æ¨¡å‹å®ç°"""

    def __init__(self, api_endpoint: str, api_key: str):
        self.api_endpoint = api_endpoint
        self.api_key = api_key

    async def request(
        self,
        messages: list[ModelMessage]
    ) -> ModelResponse:
        """å¤„ç†è¯·æ±‚"""
        # å®ç°ä¸è‡ªå®šä¹‰ API çš„äº¤äº’
        response = await self._call_api(messages)
        return ModelResponse(
            content=response['text'],
            role='assistant'
        )

    async def _call_api(self, messages):
        """è°ƒç”¨è‡ªå®šä¹‰ API"""
        # å®ç° HTTP è¯·æ±‚é€»è¾‘
        pass
```

## ğŸ¯ æ¨¡å‹é€‰æ‹©æŒ‡å—

### æŒ‰ç”¨ä¾‹é€‰æ‹©

#### é€šç”¨å¯¹è¯
- **æ¨è**: `openai:gpt-4`, `anthropic:claude-3-5-sonnet`
- **æ€§ä»·æ¯”**: `openai:gpt-3.5-turbo`, `groq:llama3-70b`

#### ä»£ç ç”Ÿæˆ
- **æœ€ä½³**: `openai:gpt-4o`, `anthropic:claude-3-5-sonnet`
- **æœ¬åœ°**: `ollama:codellama`

#### æ•°æ®åˆ†æ
- **æ¨è**: `openai:gpt-4`, `anthropic:claude-3-opus`
- **å¿«é€Ÿ**: `google:gemini-1.5-flash`

#### å¿«é€ŸåŸå‹
- **å…è´¹å¿«é€Ÿ**: `groq:llama3-70b-8192`
- **æœ¬åœ°**: `ollama:llama3`

### æŒ‰æ€§èƒ½ç‰¹ç‚¹é€‰æ‹©

| æ¨¡å‹ | é€Ÿåº¦ | è´¨é‡ | æˆæœ¬ | ä¸Šä¸‹æ–‡é•¿åº¦ |
|------|------|------|------|-----------|
| GPT-4 | â­â­ | â­â­â­â­â­ | ğŸ’°ğŸ’°ğŸ’° | 128K |
| GPT-3.5 Turbo | â­â­â­â­ | â­â­â­ | ğŸ’° | 16K |
| Claude 3.5 Sonnet | â­â­â­ | â­â­â­â­â­ | ğŸ’°ğŸ’° | 200K |
| Gemini Flash | â­â­â­â­â­ | â­â­â­ | ğŸ’° | 1M |
| Groq Llama3 | â­â­â­â­â­ | â­â­â­â­ | å…è´¹ | 8K |
| Ollama | â­â­â­ | â­â­â­ | å…è´¹ | å–å†³äºæ¨¡å‹ |

## ğŸ”„ FallbackModel

åœ¨å¤šä¸ªæ¨¡å‹ä¹‹é—´è‡ªåŠ¨åˆ‡æ¢ï¼š

```python
from pydantic_ai.models import FallbackModel, ModelSettings

fallback = FallbackModel(
    models=[
        ('openai:gpt-4', ModelSettings(temperature=0.7)),
        ('anthropic:claude-3-5-sonnet', ModelSettings(temperature=0.5)),
        ('groq:llama3-70b-8192', None),  # åå¤‡é€‰é¡¹
    ]
)

agent = Agent(fallback)

# å¦‚æœ GPT-4 å¤±è´¥ï¼ˆ4xx/5xxï¼‰ï¼Œè‡ªåŠ¨å°è¯• Claude
# å¦‚æœ Claude ä¹Ÿå¤±è´¥ï¼Œå°è¯• Groq
result = await agent.run('æŸ¥è¯¢')
```

## ğŸ§ª æµ‹è¯•æ¨¡å‹

### TestModel

ç”¨äºå•å…ƒæµ‹è¯•ï¼š

```python
from pydantic_ai.models.test import TestModel

# é¢„å®šä¹‰å“åº”
test_model = TestModel(
    custom_result='è¿™æ˜¯æµ‹è¯•å“åº”'
)

agent = Agent(test_model)
result = agent.run_sync('ä»»ä½•æç¤º')
print(result.data)  # "è¿™æ˜¯æµ‹è¯•å“åº”"

# ä½¿ç”¨å‡½æ•°ç”Ÿæˆå“åº”
def generate_response(messages):
    return f"æ”¶åˆ° {len(messages)} æ¡æ¶ˆæ¯"

test_model = TestModel(custom_result=generate_response)
```

### FunctionModel

åŸºäºå‡½æ•°çš„æ¨¡å‹ï¼š

```python
from pydantic_ai.models.function import FunctionModel


async def my_model_function(messages, tools):
    """è‡ªå®šä¹‰æ¨¡å‹é€»è¾‘"""
    # åˆ†ææ¶ˆæ¯
    user_message = messages[-1].content

    # è¿”å›å“åº”
    return ModelResponse(
        content=f'å¤„ç†: {user_message}',
        tool_calls=[]
    )


function_model = FunctionModel(my_model_function)
agent = Agent(function_model)
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. ç¯å¢ƒç‰¹å®šæ¨¡å‹

```python
import os

# ç”Ÿäº§ç¯å¢ƒç”¨é«˜è´¨é‡æ¨¡å‹
# å¼€å‘ç¯å¢ƒç”¨å¿«é€Ÿ/ä¾¿å®œæ¨¡å‹
if os.getenv('ENVIRONMENT') == 'production':
    model = 'openai:gpt-4'
else:
    model = 'groq:llama3-70b-8192'  # å…è´¹ä¸”å¿«é€Ÿ

agent = Agent(model)
```

### 2. æˆæœ¬ä¼˜åŒ–

```python
# å¯¹äºç®€å•ä»»åŠ¡ä½¿ç”¨ä¾¿å®œæ¨¡å‹
simple_agent = Agent('openai:gpt-3.5-turbo')

# å¯¹äºå¤æ‚ä»»åŠ¡ä½¿ç”¨é«˜çº§æ¨¡å‹
complex_agent = Agent('openai:gpt-4')

# æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©
if task_complexity > 0.8:
    result = complex_agent.run_sync(prompt)
else:
    result = simple_agent.run_sync(prompt)
```

### 3. ä½¿ç”¨ Fallback

```python
# ä¼˜å…ˆä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼Œå¤±è´¥æ—¶é™çº§
agent = Agent(
    FallbackModel([
        ('openai:gpt-4', None),
        ('openai:gpt-3.5-turbo', None),
        ('groq:llama3-70b-8192', None)
    ])
)
```

### 4. æ¨¡å‹ç‰¹å®šé…ç½®

```python
def get_model_settings(model_name: str) -> ModelSettings:
    """æ ¹æ®æ¨¡å‹è¿”å›æœ€ä½³é…ç½®"""
    configs = {
        'gpt-4': ModelSettings(temperature=0.7, max_tokens=2000),
        'claude-3-5-sonnet': ModelSettings(temperature=0.5, max_tokens=4096),
        'gemini-1.5-flash': ModelSettings(temperature=0.8, max_tokens=8192),
    }
    return configs.get(model_name, ModelSettings())
```

## ğŸ“š ä¸‹ä¸€æ­¥

ç»§ç»­å­¦ä¹ ï¼š

1. [07_ç»“æ„åŒ–è¾“å‡º.md](./07_ç»“æ„åŒ–è¾“å‡º.md) - æ§åˆ¶æ¨¡å‹è¾“å‡ºæ ¼å¼
2. [08_æµå¼ä¼ è¾“.md](./08_æµå¼ä¼ è¾“.md) - å®æ—¶å“åº”
3. [14_æœ€ä½³å®è·µ.md](./14_æœ€ä½³å®è·µ.md) - æ¨¡å‹é€‰æ‹©ç­–ç•¥

## ğŸ”— å‚è€ƒèµ„æº

- [å®˜æ–¹æ–‡æ¡£ - Models](https://ai.pydantic.dev/models/)
- [OpenAI å®šä»·](https://openai.com/pricing)
- [Anthropic å®šä»·](https://www.anthropic.com/pricing)
- [Groq æ–‡æ¡£](https://console.groq.com/docs)
