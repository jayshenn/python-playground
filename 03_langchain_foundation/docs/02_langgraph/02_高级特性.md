# LangGraph 高级特性详解

## 概述

LangGraph 的高级特性是构建生产级 AI 应用的关键组件。这些特性使得应用能够：

- **持久化状态**：在多次交互间保持上下文和进度
- **支持人工干预**：在关键决策点暂停执行，等待人类审批
- **管理长期记忆**：跨会话存储和检索用户偏好、历史信息
- **保证可靠执行**：通过自动重试和错误恢复机制确保系统稳定性
- **实时流式响应**：提供即时反馈，改善用户体验

这些特性共同构成了一个强大的框架，使得 AI 代理能够在真实世界的生产环境中稳定运行。

---

## 1. Persistence（持久化）

### 核心概念

持久化是 LangGraph 最重要的特性之一。它允许你在图执行的各个阶段保存状态快照（checkpoint），并在需要时恢复这些状态。

**关键组件：**
- **Checkpointer**：负责保存和检索状态快照的组件
- **Thread ID**：用于标识和隔离不同会话的唯一标识符
- **Checkpoint**：包含图状态、元数据和执行历史的快照

### 工作原理

当你使用 checkpointer 编译图时，LangGraph 会在每个"超级步骤"（super-step）后自动保存 checkpoint。超级步骤通常对应于每个节点执行完成后。

**持久化的优势：**
1. **会话管理**：支持多轮对话，保持上下文
2. **故障恢复**：应用崩溃后可以从最后一个 checkpoint 恢复
3. **时间旅行**：可以回溯到任意历史状态
4. **人工干预基础**：中断和恢复执行的前提条件

### Checkpointer 类型

LangGraph 提供多种 checkpointer 实现：

| Checkpointer | 适用场景 | 特点 |
|-------------|---------|------|
| `InMemorySaver` | 开发、测试、演示 | 轻量级，不持久化到磁盘 |
| `SqliteSaver` | 小型应用、本地开发 | 基于文件的持久化 |
| `PostgresSaver` | 生产环境 | 支持并发、高可用 |
| `AsyncSqliteSaver` | 异步应用 | 异步 I/O 支持 |

### 代码示例

#### 基础持久化（InMemorySaver）

```python
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
    foo: str
    bar: Annotated[list[str], add]

def node_a(state: State):
    return {"foo": "a", "bar": ["a"]}

def node_b(state: State):
    return {"foo": "b", "bar": ["b"]}

# 创建工作流
workflow = StateGraph(State)
workflow.add_node(node_a)
workflow.add_node(node_b)
workflow.add_edge(START, "node_a")
workflow.add_edge("node_a", "node_b")
workflow.add_edge("node_b", END)

# 使用 InMemorySaver 作为 checkpointer
checkpointer = InMemorySaver()
graph = workflow.compile(checkpointer=checkpointer)

# 执行时提供 thread_id
config = {"configurable": {"thread_id": "conversation-1"}}
result = graph.invoke({"foo": ""}, config)
```

#### SQLite 持久化

```python
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.checkpoint.sqlite import SqliteSaver

def step_1(state: MessagesState):
    return {"messages": [{"role": "assistant", "content": "Step 1 complete"}]}

def step_2(state: MessagesState):
    return {"messages": [{"role": "assistant", "content": "Step 2 complete"}]}

def step_3(state: MessagesState):
    return {"messages": [{"role": "assistant", "content": "Step 3 complete"}]}

# 使用 SQLite 持久化
checkpointer = SqliteSaver.from_conn_string("checkpoints.db")

builder = StateGraph(MessagesState)
builder.add_node("step_1", step_1)
builder.add_node("step_2", step_2)
builder.add_node("step_3", step_3)
builder.add_edge(START, "step_1")
builder.add_edge("step_1", "step_2")
builder.add_edge("step_2", "step_3")
builder.add_edge("step_3", END)

graph = builder.compile(checkpointer=checkpointer)

# 执行并保存状态
config = {"configurable": {"thread_id": "workflow-123"}}
result = graph.invoke({"messages": []}, config)

# 稍后检索保存的状态
saved_state = graph.get_state(config)
print(f"Next node: {saved_state.next}")
print(f"Messages: {saved_state.values['messages']}")

# 手动更新状态
graph.update_state(
    config,
    {"messages": [{"role": "user", "content": "Manual update"}]}
)

# 查看状态历史
for checkpoint in graph.get_state_history(config):
    print(f"Checkpoint {checkpoint.config['configurable']['checkpoint_id']}: {checkpoint.values}")
```

#### PostgreSQL 持久化（生产环境）

```python
from langgraph.checkpoint.postgres import PostgresSaver

DB_URI = "postgres://user:password@localhost:5432/mydb?sslmode=disable"

with PostgresSaver.from_conn_string(DB_URI) as memory:
    graph = builder.compile(checkpointer=memory)

    config = {"configurable": {"thread_id": "user-456", "checkpoint_ns": ""}}
    result = graph.invoke({"messages": []}, config)
```

#### 异步持久化

```python
import asyncio
import aiosqlite
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

async def main():
    async with aiosqlite.connect("checkpoints.db") as conn:
        saver = AsyncSqliteSaver(conn)

        config = {"configurable": {"thread_id": "async-session-1"}}
        checkpoint = {
            "ts": "2023-05-03T10:00:00Z",
            "data": {"key": "value"},
            "id": "0c62ca34-ac19-445d-bbb0-5b4984975b2a"
        }

        saved_config = await saver.aput(config, checkpoint, {}, {})
        print(saved_config)

asyncio.run(main())
```

### 状态管理方法

| 方法 | 用途 |
|-----|------|
| `graph.get_state(config)` | 获取当前状态 |
| `graph.update_state(config, values)` | 手动更新状态 |
| `graph.get_state_history(config)` | 获取历史 checkpoint 列表 |

---

## 2. Interrupts（中断）

### 核心概念

中断机制允许图在执行过程中暂停，等待外部输入（通常是人工审批），然后继续执行。这是实现"人在回路"（Human-in-the-Loop，HITL）工作流的关键。

**关键特性：**
- 必须配合 checkpointer 使用（否则无法恢复执行）
- 通过 `interrupt()` 函数暂停执行
- 使用 `Command(resume=value)` 恢复执行
- 可以传递任意 JSON 可序列化的数据给客户端

### 工作原理

1. 节点执行到 `interrupt()` 时，会抛出 `GraphInterrupt` 异常
2. 当前状态被保存到 checkpoint
3. 客户端收到包含中断信息的特殊输出 `__interrupt__`
4. 人工提供输入后，使用 `Command(resume=value)` 继续执行
5. 被中断的节点从 `interrupt()` 返回，获取人工提供的值

### 代码示例

#### 基础中断示例

```python
import uuid
from typing import Optional
from typing_extensions import TypedDict
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.constants import START
from langgraph.graph import StateGraph
from langgraph.types import interrupt, Command

class State(TypedDict):
    """图状态"""
    foo: str
    human_value: Optional[str]
    """将通过中断更新此值"""

def node(state: State):
    # 中断执行，向客户端发送问题
    answer = interrupt("what is your age?")
    print(f"> Received input from interrupt: {answer}")
    return {"human_value": answer}

builder = StateGraph(State)
builder.add_node("node", node)
builder.add_edge(START, "node")

# 中断必须启用 checkpointer！
checkpointer = InMemorySaver()
graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": str(uuid.uuid4())}}

# 第一次执行 - 会在中断处暂停
for chunk in graph.stream({"foo": "abc"}, config):
    print(chunk)
# 输出: {'__interrupt__': (Interrupt(value='what is your age?', ...),)}

# 人工提供输入后恢复执行
for chunk in graph.stream(Command(resume="25"), config):
    print(chunk)
# 输出:
# > Received input from interrupt: 25
# {'node': {'human_value': '25'}}
```

#### 实际应用：需要审批的工作流

```python
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import interrupt, Command

def generate_proposal(state: MessagesState):
    """生成需要审批的提案"""
    proposal = "I will delete all files in /data directory"
    return {"messages": [{"role": "assistant", "content": proposal}]}

def approval_node(state: MessagesState):
    """暂停并等待人工审批"""
    approval = interrupt(
        value={
            "question": "Do you approve this action?",
            "proposal": state["messages"][-1]["content"]
        }
    )

    if approval:
        return {"messages": [{"role": "system", "content": "Action approved"}]}
    else:
        return {"messages": [{"role": "system", "content": "Action rejected"}]}

def execute_action(state: MessagesState):
    """执行或取消操作"""
    if "approved" in state["messages"][-1]["content"]:
        return {"messages": [{"role": "assistant", "content": "Action executed"}]}
    else:
        return {"messages": [{"role": "assistant", "content": "Action cancelled"}]}

# 构建带 checkpointer 的图（中断必需）
checkpointer = InMemorySaver()
builder = StateGraph(MessagesState)
builder.add_node("proposal", generate_proposal)
builder.add_node("approval", approval_node)
builder.add_node("execute", execute_action)
builder.add_edge(START, "proposal")
builder.add_edge("proposal", "approval")
builder.add_edge("approval", "execute")
builder.add_edge("execute", END)

graph = builder.compile(checkpointer=checkpointer)

# 使用示例
config = {"configurable": {"thread_id": "approval-workflow-1"}}

# 第一阶段：执行直到审批节点
for chunk in graph.stream({"messages": []}, config):
    if "__interrupt__" in chunk:
        interrupt_data = chunk["__interrupt__"][0].value
        print(f"Question: {interrupt_data['question']}")
        print(f"Proposal: {interrupt_data['proposal']}")

# 第二阶段：提供审批决定并继续
user_approval = True  # 从 UI 或命令行获取
for chunk in graph.stream(Command(resume=user_approval), config):
    print(chunk)
```

#### 多步骤人工干预工作流

```python
import time
import uuid
from langgraph.func import entrypoint, task
from langgraph.types import interrupt, Command
from langgraph.checkpoint.memory import InMemorySaver

@task
def write_essay(topic: str) -> str:
    """撰写文章"""
    time.sleep(1)  # 模拟长时间任务
    return f"An essay about topic: {topic}"

@entrypoint(checkpointer=InMemorySaver())
def workflow(topic: str) -> dict:
    """包含人工审核的工作流"""
    essay = write_essay(topic).result()

    # 请求人工审核
    is_approved = interrupt(
        {
            "essay": essay,
            "action": "Please approve/reject the essay",
        }
    )

    return {
        "essay": essay,
        "is_approved": is_approved,
    }

thread_id = str(uuid.uuid4())
config = {"configurable": {"thread_id": thread_id}}

print("--- 初始执行 ---")
for item in workflow.stream("cats", config):
    print(item)

# 获取人工审核（例如通过 UI）
human_review = True

print("\n--- 恢复执行 ---")
for item in workflow.stream(Command(resume=human_review), config):
    print(item)
```

### 断点（Breakpoints）

除了使用 `interrupt()` 函数，还可以在编译时设置断点：

```python
# 在特定节点前设置断点
graph = builder.compile(
    checkpointer=checkpointer,
    interrupt_before=["approval_node"]  # 在此节点前暂停
)

# 或在节点后设置断点
graph = builder.compile(
    checkpointer=checkpointer,
    interrupt_after=["generate_proposal"]  # 在此节点后暂停
)
```

---

## 3. Memory（记忆）

### 核心概念

LangGraph 支持两种类型的记忆：

1. **短期记忆（Short-term Memory）**：通过 State 和 Checkpointer 实现，维护单个对话线程内的上下文
2. **长期记忆（Long-term Memory）**：通过 Store 实现，跨会话、跨线程存储持久化信息

### 短期记忆 vs 长期记忆

| 特性 | 短期记忆 | 长期记忆 |
|-----|---------|---------|
| 实现方式 | State + Checkpointer | Store |
| 作用域 | 单个对话线程（thread） | 跨多个线程 |
| 生命周期 | 会话期间 | 永久存储 |
| 用途 | 对话上下文、执行状态 | 用户偏好、历史事实、系统配置 |
| 更新时机 | 每个节点执行后 | 显式调用 `store.put()` |

### Store 的使用

Store 提供了语义搜索能力，可以通过向量相似度检索相关记忆。

#### 初始化 Store

```python
from langgraph.store.memory import InMemoryStore
from langchain.embeddings import init_embeddings

# 基础初始化
store = InMemoryStore()

# 带语义搜索的初始化
store = InMemoryStore(
    index={
        "embed": init_embeddings("openai:text-embedding-3-small"),  # 嵌入模型
        "dims": 1536,                                               # 向量维度
        "fields": ["food_preference", "$"]                          # 要嵌入的字段
    }
)
```

#### 存储和检索记忆

```python
import uuid

# 定义命名空间（用于组织记忆）
user_id = "user_123"
application_context = "preferences"
namespace = (user_id, application_context)

# 存储记忆
store.put(
    namespace,
    "memory-1",
    {
        "food_preference": "I love Italian cuisine",
        "context": "Discussing dinner plans"
    },
    index=["food_preference"]  # 仅嵌入此字段用于语义搜索
)

# 存储不嵌入的数据（仍可检索，但不可语义搜索）
store.put(
    namespace,
    "system-info",
    {"system_info": "Last updated: 2024-01-01"},
    index=False
)

# 按 ID 获取
item = store.get(namespace, "memory-1")
print(item.value)

# 语义搜索
items = store.search(
    namespace,
    query="food preferences",              # 搜索查询
    filter={"context": "Discussing dinner plans"},  # 过滤条件
    limit=5
)
```

#### 在代理中使用 Store

```python
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool
from langgraph.config import get_store
from langgraph.prebuilt import create_react_agent
from langgraph.store.memory import InMemoryStore

store = InMemoryStore()

# 预先填充用户数据
store.put(
    ("users",),
    "user_123",
    {
        "name": "John Smith",
        "language": "English",
        "preferences": {
            "communication_style": "concise",
            "topics_of_interest": ["AI", "technology"]
        }
    }
)

@tool
def get_user_info(config: RunnableConfig) -> str:
    """查询用户信息"""
    store = get_store()  # 获取代理的 store
    user_id = config["configurable"].get("user_id")
    user_info = store.get(("users",), user_id)
    return str(user_info.value) if user_info else "Unknown user"

@tool
def save_user_preference(preference: str, config: RunnableConfig) -> str:
    """保存用户偏好"""
    store = get_store()
    user_id = config["configurable"].get("user_id")

    # 生成唯一 ID
    memory_id = str(uuid.uuid4())

    store.put(
        ("users", user_id, "preferences"),
        memory_id,
        {"preference": preference, "timestamp": "2024-12-30"},
        index=["preference"]  # 启用语义搜索
    )
    return f"Saved preference: {preference}"

# 创建代理
agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_user_info, save_user_preference],
    store=store  # 传入 store
)

# 运行代理
response = agent.invoke(
    {"messages": [{"role": "user", "content": "Look up my information"}]},
    config={"configurable": {"user_id": "user_123"}}
)
```

### 记忆管理最佳实践

1. **命名空间设计**：使用层级结构组织记忆
   ```python
   # 示例：(用户ID, 应用上下文, 具体类型)
   namespace = ("user_123", "chat", "preferences")
   ```

2. **选择性嵌入**：仅嵌入需要语义搜索的字段，节省计算资源
   ```python
   store.put(namespace, key, data, index=["important_field"])
   ```

3. **生产环境使用数据库支持的 Store**：
   - `InMemoryStore` 仅用于开发和测试
   - 生产环境应使用 PostgreSQL、MongoDB 等数据库后端

---

## 4. Durable Execution（持久化执行）

### 核心概念

持久化执行是指通过 checkpointing 和自动重试机制，确保工作流即使遇到临时故障也能可靠完成。

**关键机制：**
- **Checkpointing**：在每个节点后保存状态
- **Retry Policy**：自动重试失败的节点
- **错误隔离**：单个节点失败不影响整体流程

### Retry Policy（重试策略）

LangGraph 允许为每个节点配置独立的重试策略。

#### 重试策略配置

```python
from langgraph.graph import StateGraph, START, END
from langgraph.types import RetryPolicy
from typing_extensions import TypedDict
import random

class State(TypedDict):
    attempts: int
    result: str

def unreliable_api_call(state: State):
    """模拟不稳定的 API 调用"""
    if random.random() < 0.7:  # 70% 失败率
        raise ConnectionError("API temporarily unavailable")
    return {"result": "Success!", "attempts": state.get("attempts", 0) + 1}

def reliable_operation(state: State):
    return {"result": state["result"] + " Processed"}

# 构建带重试策略的图
builder = StateGraph(State)

builder.add_node(
    "api_call",
    unreliable_api_call,
    retry_policy=RetryPolicy(
        max_attempts=5,              # 最多重试 5 次
        retry_on=ConnectionError,    # 仅在此异常时重试
        backoff_factor=2.0           # 指数退避（2、4、8、16 秒）
    )
)
builder.add_node("process", reliable_operation)

builder.add_edge(START, "api_call")
builder.add_edge("api_call", "process")
builder.add_edge("process", END)

graph = builder.compile()

try:
    result = graph.invoke({"attempts": 0})
    print(f"Success after {result['attempts']} attempts: {result['result']}")
except Exception as e:
    print(f"Failed after all retries: {e}")
```

#### 不同节点使用不同重试策略

```python
import sqlite3
from typing_extensions import TypedDict
from langchain.chat_models import init_chat_model
from langgraph.graph import END, MessagesState, StateGraph, START
from langgraph.types import RetryPolicy
from langchain_community.utilities import SQLDatabase
from langchain_core.messages import AIMessage

db = SQLDatabase.from_uri("sqlite:///:memory:")
model = init_chat_model("anthropic:claude-3-5-haiku-latest")

def query_database(state: MessagesState):
    query_result = db.run("SELECT * FROM Artist LIMIT 10;")
    return {"messages": [AIMessage(content=query_result)]}

def call_model(state: MessagesState):
    response = model.invoke(state["messages"])
    return {"messages": [response]}

# 为不同节点定义不同的重试策略
builder = StateGraph(MessagesState)

# 数据库查询节点：仅在特定错误时重试
builder.add_node(
    "query_database",
    query_database,
    retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError)
)

# LLM 调用节点：最多重试 5 次
builder.add_node(
    "model",
    call_model,
    retry_policy=RetryPolicy(max_attempts=5)
)

builder.add_edge(START, "model")
builder.add_edge("model", "query_database")
builder.add_edge("query_database", END)
graph = builder.compile()
```

#### LangChain Runnable 的重试

```python
from langchain_core.runnables import RunnableLambda
import random

def add_one(x: int) -> int:
    return x + 1

def buggy_double(y: int) -> int:
    """70% 概率失败的代码"""
    if random.random() > 0.3:
        print('Code failed, retrying...')
        raise ValueError('Triggered buggy code')
    return y * 2

# 链式调用，带重试
sequence = (
    RunnableLambda(add_one) |
    RunnableLambda(buggy_double).with_retry(
        stop_after_attempt=10,         # 最多尝试 10 次
        wait_exponential_jitter=False  # 指数退避，不加抖动
    )
)

result = sequence.invoke(2)
print(result)  # 最终会成功
```

### 故障恢复策略

1. **自动重试**：临时错误（网络问题、API 限流）
2. **Checkpointing**：系统崩溃后从最后成功的节点恢复
3. **人工介入**：无法自动解决的问题通过中断机制暂停，等待人工处理

---

## 5. Streaming（流式输出）

### 核心概念

流式输出允许实时接收图执行过程中的中间结果，而不是等待整个执行完成。这对于提供即时反馈至关重要。

### 流式模式

LangGraph 支持多种流式模式，可以组合使用：

| 模式 | 描述 | 用途 |
|-----|------|------|
| `"updates"` | 每个节点执行后的状态更新 | 查看每个节点的输出 |
| `"values"` | 完整的图状态 | 查看当前完整状态 |
| `"messages"` | LLM 生成的 token | 实现打字机效果 |
| `"custom"` | 自定义事件 | 应用特定的流式数据 |
| `"debug"` | 调试信息 | 开发和故障排查 |

### 代码示例

#### 基础流式输出

```python
from langgraph.graph import StateGraph, START, END, MessagesState
from langchain_openai import ChatOpenAI

def call_model(state: MessagesState):
    model = ChatOpenAI(model="gpt-4", streaming=True)
    response = model.invoke(state["messages"])
    return {"messages": [response]}

builder = StateGraph(MessagesState)
builder.add_node("model", call_model)
builder.add_edge(START, "model")
builder.add_edge("model", END)
graph = builder.compile()

# 流式输出节点更新
print("=== 流式输出节点更新 ===")
for chunk in graph.stream(
    {"messages": [{"role": "user", "content": "Tell me a joke"}]},
    stream_mode="updates"
):
    print(chunk)

# 流式输出 LLM tokens（打字机效果）
print("\n=== 流式输出 Tokens ===")
for message, metadata in graph.stream(
    {"messages": [{"role": "user", "content": "Count to 5"}]},
    stream_mode="messages"
):
    if hasattr(message, "content"):
        print(message.content, end="", flush=True)
```

#### 多模式流式输出

```python
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_weather],
)

# 同时流式输出多种模式
for stream_mode, chunk in agent.stream(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]},
    stream_mode=["updates", "messages", "custom"]  # 组合多种模式
):
    print(f"Mode: {stream_mode}")
    print(chunk)
    print("\n")
```

#### 异步流式输出

```python
async def stream_agent():
    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[get_weather],
    )

    async for stream_mode, chunk in agent.astream(
        {"messages": [{"role": "user", "content": "what is the weather in sf"}]},
        stream_mode=["updates", "messages"]
    ):
        print(f"[{stream_mode}] {chunk}")

# 运行异步函数
import asyncio
asyncio.run(stream_agent())
```

#### Token 级别流式输出

```python
from dataclasses import dataclass
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START

@dataclass
class MyState:
    topic: str
    joke: str = ""

llm = init_chat_model(model="openai:gpt-4o-mini")

def call_model(state: MyState):
    """调用 LLM 生成笑话"""
    llm_response = llm.invoke(
        [{"role": "user", "content": f"Generate a joke about {state.topic}"}]
    )
    return {"joke": llm_response.content}

graph = (
    StateGraph(MyState)
    .add_node(call_model)
    .add_edge(START, "call_model")
    .compile()
)

# 逐 token 流式输出
for message_chunk, metadata in graph.stream(
    {"topic": "ice cream"},
    stream_mode="messages",
):
    if message_chunk.content:
        print(message_chunk.content, end="|", flush=True)
```

### 流式输出最佳实践

1. **选择合适的流式模式**：
   - 聊天应用：使用 `"messages"` 获得打字机效果
   - 调试：使用 `"debug"` 查看详细执行信息
   - 监控：使用 `"updates"` 跟踪每个节点的输出

2. **组合多种模式**：可以同时流式输出多种信息
   ```python
   stream_mode=["updates", "messages", "custom"]
   ```

3. **处理流式数据**：
   ```python
   for chunk in graph.stream(...):
       if isinstance(chunk, dict):
           # 处理节点更新
           pass
       elif hasattr(chunk, 'content'):
           # 处理消息 token
           print(chunk.content, end="", flush=True)
   ```

4. **异步优先**：在 async 应用中使用 `astream()` 获得更好的性能

---

## 最佳实践总结

### 持久化

- ✅ 开发环境使用 `InMemorySaver`
- ✅ 生产环境使用 `PostgresSaver` 或其他数据库支持的 checkpointer
- ✅ 始终为每个会话提供唯一的 `thread_id`
- ✅ 定期清理旧的 checkpoint 以节省存储空间

### 中断与人工干预

- ✅ 中断必须配合 checkpointer 使用
- ✅ 向 `interrupt()` 传递清晰的上下文信息
- ✅ 在敏感操作（删除、修改、发送）前添加审批节点
- ✅ 使用 `interrupt_before` 或 `interrupt_after` 设置断点进行调试

### 记忆管理

- ✅ 短期记忆（对话上下文）使用 State + Checkpointer
- ✅ 长期记忆（用户偏好、事实）使用 Store
- ✅ 设计合理的命名空间层级结构
- ✅ 仅对需要语义搜索的字段进行嵌入
- ✅ 生产环境使用数据库支持的 Store

### 持久化执行

- ✅ 为不稳定的外部调用配置重试策略
- ✅ 针对不同错误类型设置不同的重试策略
- ✅ 使用指数退避避免过载下游服务
- ✅ 结合 checkpointing 实现故障恢复

### 流式输出

- ✅ 聊天应用启用 LLM streaming 和 `"messages"` 模式
- ✅ 使用多模式流式输出获取全面信息
- ✅ 异步应用优先使用 `astream()`
- ✅ 合理处理流式数据的不同类型

---

## 综合示例

以下是一个结合所有高级特性的完整示例：

```python
import uuid
from typing import Optional
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.store.memory import InMemoryStore
from langgraph.types import interrupt, Command, RetryPolicy
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.config import get_store

# ===== 定义状态 =====
class AgentState(MessagesState):
    user_id: str
    requires_approval: bool = False
    approved: Optional[bool] = None

# ===== 初始化持久化组件 =====
checkpointer = SqliteSaver.from_conn_string("agent_checkpoints.db")
store = InMemoryStore()

# 预加载用户偏好
store.put(
    ("users",),
    "user_123",
    {"name": "Alice", "risk_tolerance": "low"},
    index=["risk_tolerance"]
)

# ===== 定义工具 =====
@tool
def get_user_preferences(user_id: str) -> dict:
    """获取用户偏好"""
    store = get_store()
    user_data = store.get(("users",), user_id)
    return user_data.value if user_data else {}

# ===== 定义节点 =====
def analyze_request(state: AgentState):
    """分析用户请求"""
    user_prefs = get_user_preferences(state["user_id"])

    # 低风险承受用户需要审批
    if user_prefs.get("risk_tolerance") == "low":
        return {"requires_approval": True}
    return {"requires_approval": False}

def approval_node(state: AgentState):
    """审批节点"""
    if not state["requires_approval"]:
        return {"approved": True}

    # 中断执行，请求人工审批
    approval = interrupt({
        "question": "Approve this action?",
        "user": state["user_id"],
        "messages": state["messages"]
    })

    return {"approved": approval}

def execute_with_retry(state: AgentState):
    """带重试的执行节点"""
    if not state["approved"]:
        return {"messages": [{"role": "assistant", "content": "Action cancelled"}]}

    # 模拟可能失败的操作
    model = ChatOpenAI(model="gpt-4", streaming=True)
    response = model.invoke(state["messages"])
    return {"messages": [response]}

# ===== 构建图 =====
builder = StateGraph(AgentState)
builder.add_node("analyze", analyze_request)
builder.add_node("approval", approval_node)
builder.add_node(
    "execute",
    execute_with_retry,
    retry_policy=RetryPolicy(max_attempts=3)
)

builder.add_edge(START, "analyze")
builder.add_edge("analyze", "approval")
builder.add_edge("approval", "execute")
builder.add_edge("execute", END)

graph = builder.compile(checkpointer=checkpointer)

# ===== 使用示例 =====
thread_id = str(uuid.uuid4())
config = {"configurable": {"thread_id": thread_id}}

# 第一阶段：执行直到可能的中断点
print("=== 阶段 1: 初始执行 ===")
for stream_mode, chunk in graph.stream(
    {
        "messages": [{"role": "user", "content": "Execute trade"}],
        "user_id": "user_123"
    },
    config,
    stream_mode=["updates", "messages"]
):
    if stream_mode == "updates":
        print(f"Node update: {chunk}")
        if "__interrupt__" in chunk:
            print("⚠️  Execution paused for approval")

# 检查状态
state = graph.get_state(config)
if state.next:  # 如果有待执行的节点，说明被中断了
    print("\n=== 阶段 2: 提供审批并恢复 ===")
    for chunk in graph.stream(Command(resume=True), config):
        print(chunk)

# 最终状态
final_state = graph.get_state(config)
print(f"\n=== 最终状态 ===")
print(f"Completed: {not final_state.next}")
print(f"Messages: {final_state.values['messages']}")
```

---

## 参考链接

- [LangGraph 持久化文档](https://langchain-ai.github.io/langgraph/how-tos/persistence/)
- [LangGraph 中断和断点](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/)
- [LangGraph 记忆管理](https://langchain-ai.github.io/langgraph/how-tos/memory/)
- [LangGraph Checkpointing 概念](https://langchain-ai.github.io/langgraph/concepts/low_level/#checkpointing)
- [LangGraph 流式输出](https://langchain-ai.github.io/langgraph/how-tos/streaming/)
- [LangChain 长期记忆博客](https://blog.langchain.com/launching-long-term-memory-support-in-langgraph/)
- [LangGraph 持久化概念](https://langchain-ai.github.io/langgraph/concepts/persistence/)
