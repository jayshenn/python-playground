# LangSmith Deployment（部署）

## 概述

LangSmith Deployment 是一个托管服务，用于在生产环境中大规模运行 LangGraph 代理和应用程序。它提供了完整的部署工具链，包括从代码到生产的自动化流程、监控、扩展和管理功能。

### 核心优势

- **托管基础设施**：自动处理扩展、更新和维护
- **GitHub 集成**：直接从代码仓库部署
- **自动化 CI/CD**：构建过程完全自动化
- **内置可观测性**：集成跟踪、评估和监控
- **多环境支持**：开发环境和生产环境分离

### 部署架构

LangSmith Deployment 基于 **Agent Server**，这是一个强大的 API 服务，提供：

- **Assistant 管理**：创建和管理配置好的代理
- **持久化存储**：使用 PostgreSQL 存储会话状态
- **任务队列**：通过 Redis 管理后台任务
- **Cron 作业**：支持定时任务调度
- **Webhook 集成**：事件驱动的工作流

## 核心概念

### 1. Deployment（部署）

一个部署代表了应用程序的一个运行实例。每个部署包含：

- **代码仓库**：GitHub 上的源代码
- **配置文件**：`langgraph.json` 定义应用结构
- **环境变量**：运行时配置和密钥
- **运行环境**：开发环境或生产环境

### 2. Assistant（助手）

Assistant 是在 `langgraph.json` 中定义的已配置代理。每个 assistant：

- 有唯一的 ID（在配置文件中指定）
- 关联到一个 LangGraph graph
- 可以有自己的配置参数
- 支持多模态输入输出

### 3. Thread（会话线程）

Thread 用于管理多轮对话的状态：

- **有状态运行**：通过 thread ID 保持会话上下文
- **无状态运行**：传递 `null` 作为 thread ID，不保存状态
- **持久化**：Thread 状态存储在 PostgreSQL 中

### 4. Run（运行）

Run 代表一次代理执行：

- **同步运行**：等待完整结果
- **流式运行**：实时接收更新
- **后台运行**：异步执行，稍后获取结果

### 5. 部署类型

#### Cloud Deployment（云端部署）
- 完全托管的基础设施
- 自动扩展和负载均衡
- 开发环境：适合测试
- 生产环境：支持 500 请求/秒

#### Hybrid Deployment（混合部署）
- 控制平面在云端
- 数据平面在自己的基础设施
- 适合需要数据隐私的场景

#### Self-hosted Deployment（自托管部署）
- 完全控制所有组件
- 部署在自己的 Kubernetes 集群
- 需要手动管理扩展和维护

## 使用方法

### 1. 准备应用结构

首先创建符合 LangSmith 要求的项目结构：

```
my-app/
├── my_agent/              # 应用代码
│   ├── __init__.py
│   ├── agent.py          # Graph 定义
│   └── utils/            # 辅助模块
│       ├── __init__.py
│       ├── tools.py      # 工具函数
│       ├── nodes.py      # 节点函数
│       └── state.py      # 状态定义
├── langgraph.json        # 配置文件（必需）
├── requirements.txt      # 或 pyproject.toml
└── .env                  # 环境变量（本地使用）
```

### 2. 配置 langgraph.json

这是部署的核心配置文件：

```json
{
  "dependencies": [
    ".",                           // 当前包
    "langchain_openai",           // 外部依赖
    "../../shared-utils"          // 相对路径（monorepo 支持）
  ],
  "graphs": {
    "agent": "./my_agent/agent.py:graph",      // 主 agent
    "customer_support": "./cs_agent.py:graph"  // 多个 agents
  },
  "env": ".env"                   // 环境变量文件路径
}
```

**配置说明**：
- `dependencies`：列出所有依赖包和本地模块
- `graphs`：定义要部署的 graphs，格式为 `"id": "path/to/file.py:variable_name"`
- `env`：指向环境变量文件（部署时会被 UI 设置的变量覆盖）

### 3. 定义 Graph

创建一个完整的 LangGraph graph：

```python
# my_agent/agent.py
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI

# Define the state
class State(TypedDict):
    messages: Annotated[list, add_messages]

# Create the graph
graph_builder = StateGraph(State)

# Define the chatbot node
llm = ChatOpenAI(model="gpt-4")

def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}

# Add nodes and edges
graph_builder.add_node("chatbot", chatbot)
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)

# Compile the graph (this is what langgraph.json references)
graph = graph_builder.compile()
```

### 4. 本地测试

在部署前务必本地验证：

```bash
# 启动本地开发服务器
langgraph dev

# 在另一个终端测试 API
curl -s --request POST \
  --url http://localhost:8123/runs/stream \
  --header 'Content-Type: application/json' \
  --data '{
    "assistant_id": "agent",
    "input": {
      "messages": [{"role": "human", "content": "Hello!"}]
    },
    "stream_mode": "updates"
  }'
```

## 云端部署（Deploy to Cloud）

### 方法一：通过 LangSmith UI 部署

#### 步骤 1：准备 GitHub 仓库

1. Fork LangChain 提供的模板仓库：
   - Python: [new-langgraph-project](https://github.com/langchain-ai/new-langgraph-project)
   - JavaScript: [new-langgraphjs-project](https://github.com/langchain-ai/new-langgraphjs-project)

2. 或者将你的现有代码推送到 GitHub

#### 步骤 2：创建部署

1. 登录 [LangSmith](https://smith.langchain.com/)
2. 导航到 **Deployments** 页面
3. 点击 **+ New Deployment**
4. 完成部署配置：

**基本设置**：
```
Deployment Name: my-agent-prod
Repository: your-username/your-repo
Branch: main
Config Path: langgraph.json
```

**部署类型选择**：
- **Development**：用于测试，资源有限
- **Production**：支持高并发（500 req/s），自动扩展

**环境变量和密钥**：
```
# 敏感信息（作为 Secrets）
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...

# 非敏感配置（作为 Environment Variables）
MODEL_NAME=gpt-4
MAX_TOKENS=2000
```

**访问控制**：
- ✅ Enable Studio Access：允许通过 Studio UI 访问
- ✅ Auto-deploy on Push：代码推送时自动部署

5. 点击 **Create Deployment**

部署通常需要 10-15 分钟完成构建。

#### 步骤 3：获取 API 端点

部署成功后，在部署详情页获取：

```
Deployment URL: https://your-deployment.langchain.com
API Key: 使用你的 LangSmith API Key
```

### 方法二：通过 API 部署（高级）

使用 Control Plane API 进行编程式部署：

```python
import requests
import os

LANGSMITH_API_KEY = os.environ["LANGSMITH_API_KEY"]
WORKSPACE_ID = os.environ["LANGSMITH_WORKSPACE_ID"]

headers = {
    "X-Api-Key": LANGSMITH_API_KEY,
    "X-Tenant-Id": WORKSPACE_ID,
}

# Create deployment
deployment_data = {
    "name": "my-agent",
    "repository": "https://github.com/username/repo",
    "branch": "main",
    "config_path": "langgraph.json",
    "environment": "production",
    "env_vars": {
        "MODEL_NAME": "gpt-4"
    },
    "secrets": {
        "OPENAI_API_KEY": "sk-..."
    }
}

response = requests.post(
    "https://api.smith.langchain.com/v2/deployments",
    headers=headers,
    json=deployment_data
)

deployment = response.json()
print(f"Deployment ID: {deployment['id']}")
print(f"Status: {deployment['status']}")
```

## 部署配置（Deploy Configuration）

### 环境变量管理

#### 本地开发

在项目根目录创建 `.env` 文件：

```bash
# .env
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
TAVILY_API_KEY=tvly-...

# LangSmith tracing
LANGSMITH_API_KEY=lsv2_...
LANGSMITH_TRACING=true
LANGSMITH_PROJECT=my-project
```

**重要**：确保 `.env` 在 `.gitignore` 中！

#### 生产环境

在 LangSmith UI 中配置（推荐）：

1. 进入 Deployment Settings
2. 在 **Secrets** 部分添加敏感信息
3. 在 **Environment Variables** 添加非敏感配置

或通过 API 配置：

```python
# Update environment variables
requests.patch(
    f"https://api.smith.langchain.com/v2/deployments/{deployment_id}",
    headers=headers,
    json={
        "secrets": {
            "OPENAI_API_KEY": "new-key"
        },
        "env_vars": {
            "LOG_LEVEL": "INFO"
        }
    }
)
```

### 自定义认证

为部署添加自定义认证层：

```python
# src/security/auth.py
from langchain_core.runnables import RunnableConfig
from langgraph_sdk import Auth

async def auth(authorization: str, config: RunnableConfig) -> Auth:
    """
    Custom authentication handler

    Args:
        authorization: The Authorization header value
        config: Runtime configuration

    Returns:
        Auth object with user metadata
    """
    # Validate token (example)
    if not authorization.startswith("Bearer "):
        raise ValueError("Invalid authorization format")

    token = authorization.replace("Bearer ", "")

    # Your validation logic here
    # e.g., verify JWT, check database, etc.
    user = validate_token(token)

    return Auth(
        user_id=user["id"],
        metadata={
            "email": user["email"],
            "role": user["role"]
        }
    )
```

在 `langgraph.json` 中引用：

```json
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent.py:graph"
  },
  "auth": {
    "path": "src/security/auth.py:auth",
    "disable_studio_auth": false
  }
}
```

### Dockerfile 自定义

对于特殊依赖，可以添加自定义 Docker 指令：

```json
{
  "dockerfile_lines": [
    "RUN apt-get update && apt-get install -y libpq-dev",
    "RUN pip install psycopg2-binary"
  ],
  "dependencies": ["."],
  "graphs": {
    "agent": "./agent.py:graph"
  }
}
```

### Monorepo 支持

在 monorepo 中管理多个 agents：

```
monorepo/
├── shared-utils/
│   ├── __init__.py
│   └── common.py
├── agent-1/
│   ├── langgraph.json
│   ├── agent.py
│   └── requirements.txt
└── agent-2/
    ├── langgraph.json
    ├── agent.py
    └── requirements.txt
```

在 `agent-1/langgraph.json` 中引用共享代码：

```json
{
  "dependencies": [
    ".",
    "../shared-utils"
  ],
  "graphs": {
    "agent": "./agent.py:graph"
  }
}
```

## 代码示例

### 1. 使用 Python SDK

#### 异步调用（推荐）

```python
from langgraph_sdk import get_client

# Initialize client
client = get_client(
    url="https://your-deployment.langchain.com",
    api_key="your-langsmith-api-key"
)

# Async streaming
async def chat_with_agent(message: str):
    """Stream responses from deployed agent"""
    async for chunk in client.runs.stream(
        thread_id=None,  # None for stateless runs
        assistant_id="agent",  # From langgraph.json
        input={
            "messages": [
                {"role": "human", "content": message}
            ]
        },
        stream_mode="updates"  # or "messages", "values", "debug"
    ):
        print(f"Event: {chunk.event}")
        print(f"Data: {chunk.data}\n")

# Usage
import asyncio
asyncio.run(chat_with_agent("What is LangGraph?"))
```

#### 同步调用

```python
from langgraph_sdk import get_sync_client

# Initialize sync client
client = get_sync_client(
    url="https://your-deployment.langchain.com",
    api_key="your-langsmith-api-key"
)

# Synchronous streaming
for chunk in client.runs.stream(
    thread_id=None,
    assistant_id="agent",
    input={
        "messages": [
            {"role": "human", "content": "Explain quantum computing"}
        ]
    },
    stream_mode="updates"
):
    print(f"Chunk: {chunk.data}")
```

#### 有状态对话

```python
# Create a thread for persistent conversation
thread = await client.threads.create()
thread_id = thread["thread_id"]

# First message
async for chunk in client.runs.stream(
    thread_id=thread_id,
    assistant_id="agent",
    input={"messages": [{"role": "human", "content": "My name is Alice"}]},
    stream_mode="updates"
):
    print(chunk.data)

# Follow-up message (remembers context)
async for chunk in client.runs.stream(
    thread_id=thread_id,
    assistant_id="agent",
    input={"messages": [{"role": "human", "content": "What's my name?"}]},
    stream_mode="updates"
):
    print(chunk.data)

# Get conversation history
history = await client.threads.get_history(thread_id)
print(history)
```

### 2. 使用 JavaScript/TypeScript SDK

```typescript
import { Client } from "@langchain/langgraph-sdk";

// Initialize client
const client = new Client({
  apiUrl: "https://your-deployment.langchain.com",
  apiKey: "your-langsmith-api-key"
});

// Streaming example
async function chatWithAgent(message: string) {
  const streamResponse = client.runs.stream(
    null,  // threadless run
    "agent",  // assistant ID
    {
      input: {
        messages: [
          { role: "user", content: message }
        ]
      },
      streamMode: "messages"
    }
  );

  for await (const chunk of streamResponse) {
    console.log(`Event: ${chunk.event}`);
    console.log(`Data:`, JSON.stringify(chunk.data, null, 2));
  }
}

// Usage
chatWithAgent("What is LangGraph?");
```

### 3. 使用 REST API

#### cURL 示例

```bash
# Streaming request
curl -X POST \
  https://your-deployment.langchain.com/runs/stream \
  -H "Content-Type: application/json" \
  -H "X-Api-Key: your-langsmith-api-key" \
  -d '{
    "assistant_id": "agent",
    "input": {
      "messages": [
        {
          "role": "human",
          "content": "What is LangGraph?"
        }
      ]
    },
    "stream_mode": "updates"
  }'
```

#### Python requests

```python
import requests
import json

def call_agent(message: str, deployment_url: str, api_key: str):
    """Call deployed agent via REST API"""
    response = requests.post(
        f"{deployment_url}/runs/stream",
        headers={
            "Content-Type": "application/json",
            "X-Api-Key": api_key
        },
        json={
            "assistant_id": "agent",
            "input": {
                "messages": [
                    {"role": "human", "content": message}
                ]
            },
            "stream_mode": "updates"
        },
        stream=True  # Important for streaming
    )

    # Process streaming response
    for line in response.iter_lines():
        if line:
            data = json.loads(line)
            print(f"Event: {data['event']}")
            print(f"Data: {data['data']}\n")

# Usage
call_agent(
    "Explain machine learning",
    "https://your-deployment.langchain.com",
    "your-api-key"
)
```

### 4. 不同的 Stream Modes

```python
# "updates" - 每个节点的输出
stream_mode="updates"
# Output: {"chatbot": {"messages": [...]}}

# "messages" - 只返回消息更新
stream_mode="messages"
# Output: [("chatbot", AIMessage(...))]

# "values" - 完整的状态快照
stream_mode="values"
# Output: {"messages": [...complete state...]}

# "debug" - 详细调试信息
stream_mode="debug"
# Output: Full execution details including node names and states
```

### 5. 后台运行（Background Runs）

```python
# Create a background run
run = await client.runs.create(
    thread_id=thread_id,
    assistant_id="agent",
    input={"messages": [{"role": "human", "content": "Long task"}]}
)

run_id = run["run_id"]

# Check status
status = await client.runs.get(thread_id, run_id)
print(f"Status: {status['status']}")  # "pending", "running", "success", "error"

# Wait for completion
await client.runs.join(thread_id, run_id)

# Get results
result = await client.runs.get(thread_id, run_id)
print(result["output"])
```

### 6. Cron 作业

```python
# Schedule a recurring task
cron = await client.crons.create(
    assistant_id="agent",
    schedule="0 9 * * *",  # Daily at 9 AM
    input={
        "messages": [
            {"role": "human", "content": "Generate daily report"}
        ]
    }
)

print(f"Cron job created: {cron['cron_id']}")

# List all cron jobs
crons = await client.crons.list()

# Delete a cron job
await client.crons.delete(cron["cron_id"])
```

## 生产环境最佳实践

### 1. 性能优化

#### 使用无状态运行减少延迟

```python
# 对于不需要记忆的场景，使用 threadless runs
async for chunk in client.runs.stream(
    thread_id=None,  # No persistence overhead
    assistant_id="agent",
    input=input_data,
    stream_mode="updates"
):
    process(chunk)
```

#### 实现请求批处理

```python
async def batch_process(messages: list[str]):
    """Process multiple requests concurrently"""
    tasks = [
        client.runs.stream(None, "agent", {"messages": [{"role": "human", "content": msg}]})
        for msg in messages
    ]
    results = await asyncio.gather(*tasks)
    return results
```

#### 配置超时和重试

```python
from langgraph_sdk import get_client

client = get_client(
    url=deployment_url,
    api_key=api_key,
    timeout=30.0,  # Request timeout in seconds
)

# Implement retry logic
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
async def robust_call(input_data):
    return await client.runs.stream(None, "agent", input_data)
```

### 2. 安全最佳实践

#### 密钥管理

```python
# ❌ 错误：硬编码密钥
client = get_client(
    url="https://...",
    api_key="lsv2_pt_abc123..."  # Never do this!
)

# ✅ 正确：使用环境变量
import os
client = get_client(
    url=os.environ["DEPLOYMENT_URL"],
    api_key=os.environ["LANGSMITH_API_KEY"]
)
```

#### 输入验证

```python
from pydantic import BaseModel, Field, validator

class UserInput(BaseModel):
    """Validate user input before sending to agent"""
    message: str = Field(..., min_length=1, max_length=1000)
    user_id: str

    @validator("message")
    def sanitize_message(cls, v):
        # Remove potentially harmful content
        forbidden = ["<script>", "DROP TABLE", "rm -rf"]
        for term in forbidden:
            if term.lower() in v.lower():
                raise ValueError(f"Forbidden content detected")
        return v

# Usage
user_input = UserInput(message=request.message, user_id=request.user_id)
result = await client.runs.stream(None, "agent", {"messages": [...]})
```

#### 实现速率限制

```python
from datetime import datetime, timedelta
from collections import defaultdict

class RateLimiter:
    def __init__(self, max_requests: int, window_seconds: int):
        self.max_requests = max_requests
        self.window = timedelta(seconds=window_seconds)
        self.requests = defaultdict(list)

    def allow_request(self, user_id: str) -> bool:
        now = datetime.now()
        # Clean old requests
        self.requests[user_id] = [
            req_time for req_time in self.requests[user_id]
            if now - req_time < self.window
        ]
        # Check limit
        if len(self.requests[user_id]) >= self.max_requests:
            return False
        self.requests[user_id].append(now)
        return True

# Usage
limiter = RateLimiter(max_requests=10, window_seconds=60)

if not limiter.allow_request(user_id):
    raise Exception("Rate limit exceeded")
```

### 3. 监控和告警

#### 配置 LangSmith 追踪

```python
import os

# Enable tracing for deployed agents
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_PROJECT"] = "production-agent"
os.environ["LANGSMITH_API_KEY"] = "your-key"

# All runs will automatically appear in LangSmith dashboard
```

#### 自定义日志和指标

```python
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

async def monitored_run(input_data: dict):
    """Run with monitoring"""
    start_time = datetime.now()

    try:
        result = []
        async for chunk in client.runs.stream(None, "agent", input_data):
            result.append(chunk)

        # Log success metrics
        duration = (datetime.now() - start_time).total_seconds()
        logger.info(f"Run completed in {duration}s")

        return result

    except Exception as e:
        # Log errors
        logger.error(f"Run failed: {str(e)}", exc_info=True)

        # Send alert (integrate with your monitoring system)
        send_alert(f"Agent failure: {str(e)}")
        raise
```

#### 使用 Dashboard

在 LangSmith UI 中设置监控：

1. **Traces**：查看每次运行的详细执行路径
2. **Dashboards**：监控关键指标
   - 请求量
   - 错误率
   - P95/P99 延迟
   - Token 使用量
3. **Alerts**：配置告警规则
   - 错误率超过阈值
   - 延迟异常
   - 成本超标

### 4. 错误处理

#### 优雅降级

```python
async def resilient_agent_call(input_data: dict, fallback_response: str = None):
    """Agent call with fallback"""
    try:
        async for chunk in client.runs.stream(
            None,
            "agent",
            input_data,
            stream_mode="updates"
        ):
            yield chunk

    except Exception as e:
        logger.error(f"Agent error: {e}")

        if fallback_response:
            # Return fallback response
            yield {"error": False, "message": fallback_response}
        else:
            # Return error to user
            yield {"error": True, "message": "Service temporarily unavailable"}
```

#### 状态检查

```python
async def health_check(deployment_url: str, api_key: str) -> bool:
    """Check if deployment is healthy"""
    try:
        # Use the /ok endpoint (if available)
        response = requests.get(
            f"{deployment_url}/ok",
            headers={"X-Api-Key": api_key},
            timeout=5
        )
        return response.status_code == 200
    except:
        return False
```

### 5. 成本优化

#### Token 使用跟踪

```python
from langchain_core.callbacks import BaseCallbackHandler

class TokenCounterCallback(BaseCallbackHandler):
    """Track token usage"""

    def __init__(self):
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.total_cost = 0.0

    def on_llm_end(self, response, **kwargs):
        usage = response.llm_output.get("token_usage", {})
        self.prompt_tokens += usage.get("prompt_tokens", 0)
        self.completion_tokens += usage.get("completion_tokens", 0)

        # Calculate cost (example rates)
        self.total_cost += (
            self.prompt_tokens * 0.00001 +  # $0.01 per 1K tokens
            self.completion_tokens * 0.00003  # $0.03 per 1K tokens
        )
```

#### 缓存策略

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
async def cached_agent_call(input_hash: str, input_data: str):
    """Cache frequent queries"""
    data = json.loads(input_data)
    result = await client.runs.stream(None, "agent", data)
    return result

# Usage
input_data = {"messages": [...]}
input_hash = hashlib.md5(json.dumps(input_data).encode()).hexdigest()
result = await cached_agent_call(input_hash, json.dumps(input_data))
```

### 6. 部署策略

#### 蓝绿部署

```python
# Maintain two deployments
BLUE_DEPLOYMENT = "https://agent-v1.langchain.com"
GREEN_DEPLOYMENT = "https://agent-v2.langchain.com"

# Route traffic based on feature flag
def get_deployment_url(user_id: str) -> str:
    if should_use_new_version(user_id):
        return GREEN_DEPLOYMENT
    return BLUE_DEPLOYMENT

# Gradually shift traffic
client = get_client(
    url=get_deployment_url(user_id),
    api_key=api_key
)
```

#### 金丝雀发布

```python
import random

def get_deployment_with_canary(canary_percentage: float = 10.0) -> str:
    """Route small percentage to new version"""
    if random.random() * 100 < canary_percentage:
        return CANARY_DEPLOYMENT
    return STABLE_DEPLOYMENT
```

## 常见问题

### Q1: 部署需要多长时间？

**A**: 通常 10-15 分钟。首次部署需要：
- 拉取代码仓库（1-2 分钟）
- 安装依赖（3-8 分钟）
- 构建 Docker 镜像（3-5 分钟）
- 启动服务（1-2 分钟）

后续更新如果依赖未变化会更快（5-10 分钟）。

### Q2: 如何更新已部署的应用？

**A**: 三种方式：

1. **自动部署**（推荐）：
   ```bash
   # 在部署设置中启用 "Auto-deploy on Push"
   git push origin main
   # 自动触发重新部署
   ```

2. **手动触发**：
   - 在 LangSmith UI 点击 "Create New Revision"

3. **通过 API**：
   ```python
   requests.post(
       f"https://api.smith.langchain.com/v2/deployments/{deployment_id}/revisions",
       headers=headers
   )
   ```

### Q3: 如何调试部署失败？

**A**:
1. 检查 **Build Logs**（构建日志）
2. 检查 **Server Logs**（服务器日志）
3. 常见问题：
   - 依赖版本冲突：检查 `requirements.txt`
   - 环境变量缺失：确认所有必需的 secrets 已设置
   - 代码错误：本地运行 `langgraph dev` 测试
   - 路径错误：确认 `langgraph.json` 中的路径正确

### Q4: 部署支持哪些 Python 版本？

**A**:
- Python 3.9+
- 推荐使用 3.11 或 3.12
- 在 `pyproject.toml` 中指定：
  ```toml
  [project]
  requires-python = ">=3.11"
  ```

### Q5: 如何处理大文件或模型？

**A**:
```python
# 选项 1: 在 Dockerfile 中下载
{
  "dockerfile_lines": [
    "RUN python -c 'from transformers import AutoModel; AutoModel.from_pretrained(\"bert-base\")'"
  ]
}

# 选项 2: 使用云存储
# 在运行时从 S3/GCS 下载模型
```

### Q6: 部署的 API 限制是什么？

**A**:
- **开发环境**：较低限制，适合测试
- **生产环境**：500 请求/秒，自动扩展
- **超时**：默认 30 秒，可配置
- **响应大小**：建议 < 10MB

### Q7: 如何实现多租户？

**A**:
```python
# 使用自定义认证获取租户信息
async def auth(authorization: str, config: RunnableConfig) -> Auth:
    user = verify_token(authorization)
    return Auth(
        user_id=user["id"],
        metadata={"tenant_id": user["tenant_id"]}
    )

# 在 graph 中访问租户信息
def node_function(state: State, config: RunnableConfig):
    tenant_id = config["configurable"].get("tenant_id")
    # Use tenant_id to fetch tenant-specific data
```

### Q8: 可以使用私有包吗？

**A**: 可以，几种方式：

1. **使用 Git 依赖**：
   ```txt
   # requirements.txt
   git+https://username:token@github.com/org/private-repo.git
   ```

2. **使用私有 PyPI**：
   ```dockerfile
   RUN pip config set global.index-url https://pypi.company.com/simple/
   ```

3. **包含在代码仓库**：
   ```json
   {
     "dependencies": ["./vendor/private_package"]
   }
   ```

### Q9: 如何监控成本？

**A**:
1. 在 LangSmith Dashboard 查看：
   - Token 使用统计
   - 请求量趋势
   - 估算成本

2. 设置预算告警
3. 使用回调跟踪每次调用的成本

### Q10: 部署的数据存储在哪里？

**A**:
- **Cloud 部署**：
  - US: `us-central1` (Iowa, GCP)
  - EU: `europe-west4` (Netherlands, GCP)
- **Hybrid/Self-hosted**：你自己的基础设施
- **Thread 状态**：PostgreSQL
- **任务队列**：Redis

## 参考链接

### 官方文档
- [LangSmith Deployment 文档](https://docs.langchain.com/langsmith/deployments)
- [LangSmith Cloud 部署](https://docs.langchain.com/langsmith/cloud)
- [Agent Server API 参考](https://docs.langchain.com/langsmith/server-api-ref)
- [Control Plane API 参考](https://docs.langchain.com/langsmith/api-ref-control-plane)

### 快速开始
- [部署快速开始指南](https://docs.langchain.com/langsmith/deployment-quickstart)
- [应用结构配置](https://docs.langchain.com/langsmith/application-structure)
- [LangGraph CLI 文档](https://docs.langchain.com/langsmith/cli)

### 配置和管理
- [环境变量配置](https://docs.langchain.com/langsmith/deploy-to-cloud)
- [自定义认证](https://docs.langchain.com/langsmith/set-up-custom-auth)
- [Monorepo 支持](https://docs.langchain.com/langsmith/monorepo-support)

### 监控和调试
- [Dashboard 和监控](https://docs.langchain.com/langsmith/dashboards)
- [故障排查指南](https://docs.langchain.com/langsmith/troubleshooting)
- [LangSmith 追踪](https://docs.langchain.com/langsmith/trace-with-langgraph)

### 示例项目
- [Python 模板项目](https://github.com/langchain-ai/new-langgraph-project)
- [JavaScript 模板项目](https://github.com/langchain-ai/new-langgraphjs-project)

### SDK 文档
- [LangGraph SDK (Python)](https://langchain-ai.github.io/langgraph/sdk/)
- [LangGraph SDK (JavaScript)](https://www.npmjs.com/package/@langchain/langgraph-sdk)
