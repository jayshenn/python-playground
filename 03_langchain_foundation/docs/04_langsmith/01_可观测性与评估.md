# LangSmith 可观测性与评估

## 概述

LangSmith 是 LangChain 团队开发的生产级 LLM 应用开发平台，提供了强大的**可观测性（Observability）**和**评估（Evaluation）**能力。这两个核心功能帮助开发者在整个应用生命周期中监控、调试和改进 LLM 应用。

### 什么是 LangSmith？

LangSmith 是一个为 LLM 应用量身定制的开发平台，提供：

- **LLM 原生可观测性**：深入了解应用在原型开发到生产环境各阶段的运行状况
- **全面追踪系统**：记录应用执行的层次化详细信息
- **智能评估框架**：支持离线和在线评估，持续优化应用质量
- **性能监控**：实时跟踪延迟、token 使用量、成本等关键指标
- **AI 助手 Polly**：利用 AI 分析追踪数据，提供智能洞察

### 为什么需要可观测性和评估？

在开发 LLM 应用时，我们面临独特的挑战：

1. **非确定性输出**：相同输入可能产生不同输出，难以预测和测试
2. **复杂的执行流程**：涉及多个 LLM 调用、工具使用、数据检索等步骤
3. **性能优化难题**：需要平衡质量、延迟和成本
4. **生产环境监控**：实时检测问题，收集用户反馈

LangSmith 的可观测性和评估功能正是为了解决这些挑战而设计。

---

## 第一部分：可观测性（Observability）

### 核心概念

#### 1. Trace（追踪）

**Trace** 是 LangSmith 可观测性的核心概念，它是应用执行过程的层次化记录。每个 trace 包含：

- **Run（运行）**：应用中的单个操作单元
- **Parent-Child 关系**：形成树状结构，反映执行流程
- **元数据**：输入、输出、时间戳、token 使用量、错误信息等

**Run 类型**：

- `chain`：链式调用或工作流
- `llm`：大语言模型调用
- `tool`：工具或函数调用
- `retriever`：检索操作
- `agent`：智能代理决策

#### 2. Project（项目）

Project 是 trace 的逻辑分组，用于组织不同环境或版本的追踪数据：

- 开发环境：`my-app-dev`
- 测试环境：`my-app-test`
- 生产环境：`my-app-prod`

#### 3. Metadata 和 Tags

- **Metadata**：键值对形式的附加信息（如版本号、用户 ID）
- **Tags**：用于分类和过滤的标签（如 `["production", "v2.0"]`）

### 环境配置

在使用 LangSmith 之前，需要配置环境变量：

```python
import os

# Enable LangSmith tracing
os.environ["LANGSMITH_TRACING"] = "true"

# Set API endpoint (default: US region)
os.environ["LANGSMITH_ENDPOINT"] = "https://api.smith.langchain.com"
# For EU region:
# os.environ["LANGSMITH_ENDPOINT"] = "https://eu.api.smith.langchain.com"

# Set your API key (get from https://smith.langchain.com)
os.environ["LANGSMITH_API_KEY"] = "ls_..."

# Optional: Set project name (default: "default")
os.environ["LANGSMITH_PROJECT"] = "my-awesome-project"

# Optional: For org-scoped API keys
# os.environ["LANGSMITH_WORKSPACE_ID"] = "<your-workspace-id>"
```

**获取 API Key**：

1. 访问 https://smith.langchain.com
2. 登录或注册账户
3. 进入 Settings → API Keys
4. 创建新的 API Key

### 使用方法

#### 方法 1：使用 @traceable 装饰器（推荐）

`@traceable` 装饰器是最简单的追踪方式，支持同步、异步和生成器函数：

```python
from langsmith import traceable
import asyncio
import httpx

# Basic usage - synchronous function
@traceable
def my_function(x: float, y: float) -> float:
    """Simple traced function"""
    return x + y

result = my_function(5, 6)
# Automatically traced with inputs (x=5, y=6) and output (11)

# Async function tracing
@traceable
async def my_async_function(query_params: dict) -> dict:
    """Traced async function with external API call"""
    async with httpx.AsyncClient() as http_client:
        response = await http_client.get(
            "https://api.example.com/data",
            params=query_params,
        )
        return response.json()

result = asyncio.run(my_async_function({"param": "value"}))

# Advanced: Custom configuration
@traceable(
    name="CustomName",           # Custom trace name
    run_type="tool",              # Specify run type
    metadata={"version": "1.0"},  # Add metadata
    tags=["production", "v2"]     # Add tags
)
def tagged_function(a: float, b: float) -> float:
    """Traced function with custom configuration"""
    return a * b

result = tagged_function(5, 6)
```

**装饰器参数说明**：

- `name`：自定义追踪名称（默认使用函数名）
- `run_type`：运行类型（`"chain"`, `"llm"`, `"tool"` 等）
- `metadata`：元数据字典
- `tags`：标签列表
- `project_name`：指定项目名称（覆盖环境变量）
- `client`：自定义 LangSmith Client 实例

#### 方法 2：使用 trace 上下文管理器

对于更复杂的场景，可以使用 `trace` 上下文管理器：

```python
from langsmith import trace, traceable, Client

@traceable
def foo(x):
    return {"y": x * 2}

@traceable
def bar(y):
    return {"z": y - 1}

client = Client()

inputs = {"x": 1}
with trace(name="foobar", run_type="chain", inputs=inputs) as root_run:
    # Execute your workflow
    result = foo(**inputs)
    result = bar(**result)

    # Set outputs
    root_run.outputs = result

    # Access trace information
    trace_id = root_run.id
    child_runs = root_run.child_runs

    # Add metadata dynamically
    root_run.metadata["user_id"] = "user-123"
```

**上下文管理器的优势**：

- 显式控制追踪的开始和结束
- 动态添加元数据
- 访问追踪 ID 和子运行
- 适合复杂的手动工作流

#### 方法 3：集成 OpenAI/Anthropic 客户端

LangSmith 提供了包装器（Wrapper）来自动追踪 LLM API 调用：

```python
from openai import OpenAI
from langsmith import traceable
from langsmith.wrappers import wrap_openai
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = "ls_..."
os.environ["OPENAI_API_KEY"] = "sk-..."

# Wrap OpenAI client for automatic tracing
client = wrap_openai(OpenAI())

# All calls are automatically traced with token usage and latency
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is machine learning?"}
    ],
    temperature=0.7
)

print(response.choices[0].message.content)
# Trace available at: https://smith.langchain.com/...
```

**支持的客户端包装器**：

- `wrap_openai()` - OpenAI
- `wrap_gemini()` - Google Gemini
- LangChain/LangGraph 自动集成

**Google Gemini 示例**：

```python
import google.generativeai as genai
from langsmith import traceable
from langsmith.wrappers import wrap_gemini
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = "ls_..."
os.environ["GOOGLE_API_KEY"] = "..."

# Configure and wrap Gemini
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
client = wrap_gemini(genai.GenerativeModel("gemini-1.5-flash"))

# Direct usage - automatically traced
response = client.generate_content("What is the capital of France?")
print(response.text)

# Nested tracing with custom functions
@traceable(name="gemini_qa_pipeline")
def qa_system(question: str) -> str:
    """Question answering with Gemini"""
    response = client.generate_content(question)
    return response.text

result = qa_system("Explain quantum computing in simple terms")
print(result)
```

#### 方法 4：与 LangChain 集成

如果使用 LangChain，只需设置环境变量即可自动追踪：

```python
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Configure LangSmith (LangChain will auto-detect)
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = "ls_..."
os.environ["LANGSMITH_PROJECT"] = "langchain-integration"

# Create a simple chain
llm = ChatOpenAI(model="gpt-4")
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful coding assistant."),
    ("user", "{input}")
])
output_parser = StrOutputParser()

chain = prompt | llm | output_parser

# All invocations are automatically traced
result = chain.invoke({"input": "How do I reverse a string in Python?"})
print(result)
```

### 监控和调试

#### 1. 在 UI 中查看 Traces

访问 https://smith.langchain.com 查看追踪数据：

- **Trace Tree View**：层次化显示所有运行
- **Input/Output Inspection**：查看每个步骤的输入输出
- **Performance Metrics**：延迟、token 使用量、成本
- **Error Tracking**：自动捕获异常和错误

**过滤和搜索**：

- 按项目、时间范围、标签过滤
- 搜索特定输入或输出内容
- 按错误状态筛选
- 按性能指标排序

#### 2. 使用 Polly AI 助手

Polly 是 LangSmith 的 AI 助手，可以：

- 分析 trace 模式
- 识别性能瓶颈
- 提供优化建议
- 解释错误原因

**使用方式**：在 trace 页面点击 "Ask Polly" 按钮。

#### 3. 创建监控仪表板

LangSmith 支持创建自定义仪表板：

```python
# Example: Monitor key metrics programmatically
from langsmith import Client

client = Client()

# List recent runs with filters
runs = client.list_runs(
    project_name="my-app-prod",
    start_time="2025-12-01",
    end_time="2025-12-30",
    filter='eq(status, "success")'
)

# Calculate metrics
total_runs = len(list(runs))
avg_latency = sum(run.latency for run in runs) / total_runs if total_runs > 0 else 0

print(f"Total successful runs: {total_runs}")
print(f"Average latency: {avg_latency:.2f}s")
```

#### 4. 设置告警

在 LangSmith UI 中配置告警规则：

- 错误率超过阈值
- 延迟超过阈值
- Token 使用量异常
- 自定义规则

### 代码示例：完整的可观测性实践

```python
import os
from langsmith import traceable, Client
from openai import OpenAI
from langsmith.wrappers import wrap_openai

# Step 1: Configure environment
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = "ls_..."
os.environ["LANGSMITH_PROJECT"] = "rag-chatbot"
os.environ["OPENAI_API_KEY"] = "sk-..."

# Step 2: Initialize clients
client = Client()
llm_client = wrap_openai(OpenAI())

# Step 3: Define traced functions
@traceable(name="retrieve_documents", run_type="retriever")
def retrieve_documents(query: str) -> list[str]:
    """Simulate document retrieval"""
    # In real app, this would query a vector database
    documents = [
        "Python is a high-level programming language.",
        "Python was created by Guido van Rossum in 1991.",
        "Python is known for its simplicity and readability."
    ]
    return documents

@traceable(name="generate_answer", run_type="llm")
def generate_answer(query: str, context: list[str]) -> str:
    """Generate answer using LLM"""
    context_str = "\n".join(context)

    response = llm_client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Answer based on the context provided."},
            {"role": "user", "content": f"Context:\n{context_str}\n\nQuestion: {query}"}
        ]
    )

    return response.choices[0].message.content

@traceable(
    name="rag_pipeline",
    run_type="chain",
    metadata={"version": "2.0"},
    tags=["production", "rag"]
)
def rag_pipeline(query: str) -> dict:
    """Complete RAG pipeline with tracing"""
    try:
        # Step 1: Retrieve documents
        documents = retrieve_documents(query)

        # Step 2: Generate answer
        answer = generate_answer(query, documents)

        return {
            "query": query,
            "answer": answer,
            "sources": len(documents),
            "status": "success"
        }
    except Exception as e:
        return {
            "query": query,
            "error": str(e),
            "status": "error"
        }

# Step 4: Execute and trace
if __name__ == "__main__":
    query = "What is Python?"
    result = rag_pipeline(query)

    print(f"Query: {result['query']}")
    print(f"Answer: {result['answer']}")
    print(f"Status: {result['status']}")

    # View trace at: https://smith.langchain.com/
```

---

## 第二部分：评估（Evaluation）

### 核心概念

#### 1. 评估类型

LangSmith 支持两种主要的评估方式：

**离线评估（Offline Evaluation）**：

- 在开发阶段使用精选数据集测试
- 用于版本比较、基准测试、回归检测
- 在部署前识别问题

**在线评估（Online Evaluation）**：

- 监控生产环境中的真实用户交互
- 实时检测质量问题
- 收集用户反馈

#### 2. Dataset（数据集）

Dataset 是评估的基础，包含一组 Example（示例）：

```python
from langsmith import Client

client = Client()

# Create a dataset
dataset = client.create_dataset(
    dataset_name="qa-test-set",
    description="Question answering evaluation dataset"
)

# Add examples
examples = [
    {
        "inputs": {"question": "What is machine learning?"},
        "outputs": {"answer": "Machine learning is a subset of AI..."},
        "metadata": {"difficulty": "easy", "category": "definition"}
    },
    {
        "inputs": {"question": "Explain neural networks"},
        "outputs": {"answer": "Neural networks are computing systems..."},
        "metadata": {"difficulty": "medium", "category": "explanation"}
    },
    {
        "inputs": {"question": "What is backpropagation?"},
        "outputs": {"answer": "Backpropagation is an algorithm..."},
        "metadata": {"difficulty": "hard", "category": "technical"}
    }
]

client.create_examples(dataset_name="qa-test-set", examples=examples)
```

**创建数据集的方法**：

1. **手动创建**：精心设计的测试用例
2. **从生产数据创建**：从真实运行中提取
3. **合成生成**：使用 LLM 生成测试数据

**从生产运行创建示例**：

```python
from langsmith import Client

client = Client()

# Get a run by ID
run = client.read_run("run-id-here")

# Add it to a dataset as an example
example = client.create_example_from_run(
    run=run,
    dataset_id="dataset-id-here"
)
```

#### 3. Evaluator（评估器）

Evaluator 定义如何评分。LangSmith 支持两种评估器：

**Row-level Evaluator（行级评估器）**：

- 评估单个示例的输出
- 返回分数或指标
- 可以是规则、启发式或 LLM 判断

**Summary Evaluator（汇总评估器）**：

- 评估整个实验的结果
- 计算聚合指标（如准确率、精确率）
- 用于整体性能评估

#### 4. Experiment（实验）

Experiment 是在数据集上运行应用并收集评估结果的过程。

### 使用方法

#### 基础评估流程

```python
import asyncio
from langsmith import Client

client = Client()

# Step 1: Define your application
async def apredict(inputs: dict) -> dict:
    """Your application to be evaluated"""
    # Simulate async processing
    await asyncio.sleep(0.1)
    question = inputs["question"]

    # Simple mock response (replace with real LLM call)
    if "machine learning" in question.lower():
        return {"response": "Yes"}
    else:
        return {"response": "No"}

# Step 2: Define evaluators
def accuracy(outputs: dict, reference_outputs: dict) -> dict:
    """Row-level evaluator for accuracy"""
    pred = outputs["response"]
    expected = reference_outputs["answer"]
    return {
        "key": "accuracy",
        "score": expected.lower() == pred.lower()
    }

def precision(outputs: list[dict], reference_outputs: list[dict]) -> dict:
    """Summary evaluator for precision"""
    # TP / (TP + FP)
    predictions = [out["response"].lower() for out in outputs]
    expected = [ref["answer"].lower() for ref in reference_outputs]

    tp = sum([p == e for p, e in zip(predictions, expected) if p == "yes"])
    fp = sum([p == "yes" and e == "no" for p, e in zip(predictions, expected)])

    if tp + fp == 0:
        return {"key": "precision", "score": 0.0}

    return {
        "key": "precision",
        "score": tp / (tp + fp)
    }

# Step 3: Clone or create dataset
dataset = client.clone_public_dataset(
    "https://smith.langchain.com/public/419dcab2-1d66-4b94-8901-0357ead390df/d"
)
dataset_name = "Evaluate Examples"

# Step 4: Run evaluation
results = asyncio.run(
    client.aevaluate(
        apredict,                       # Function to evaluate
        data=dataset_name,              # Dataset name
        evaluators=[accuracy],          # Row-level evaluators
        summary_evaluators=[precision], # Summary evaluators
        experiment_prefix="My Experiment",
        description="Evaluate the accuracy of the model asynchronously.",
        metadata={"my-prompt-version": "abcd-1234"}
    )
)

# Step 5: View results
print(f"Experiment completed!")
print(f"Results URL: {results}")
```

#### 使用 LLM 作为评估器

```python
import asyncio
from langsmith import Client

client = Client()

async def helpfulness(outputs: dict) -> dict:
    """LLM-based row-level evaluator for helpfulness"""
    # Call LLM to judge helpfulness (simplified example)
    from openai import AsyncOpenAI

    llm = AsyncOpenAI()
    response = await llm.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Rate if the answer is helpful (1) or not (0)."},
            {"role": "user", "content": f"Answer: {outputs['output']}"}
        ]
    )

    # Parse score from response
    score = 1 if "helpful" in response.choices[0].message.content.lower() else 0

    return {
        "key": "helpfulness",
        "score": score
    }

# Use in evaluation
results = asyncio.run(
    client.aevaluate(
        apredict,
        data="Evaluate Examples",
        evaluators=[helpfulness],
        experiment_prefix="LLM Judge Experiment",
        description="Using LLM as evaluator."
    )
)
```

#### 使用 LangChain 内置评估器

```python
from langsmith import Client
from langsmith.evaluation import LangChainStringEvaluator
from langchain_openai import ChatOpenAI

client = Client()

def prepare_criteria_data(run, example):
    """Prepare data for criteria evaluator"""
    return {
        "prediction": run.outputs["output"],
        "reference": example.outputs["answer"],
        "input": str(example.inputs)
    }

results = client.evaluate(
    predict,
    data=dataset_name,
    evaluators=[
        # Embedding similarity
        LangChainStringEvaluator("embedding_distance"),

        # Custom criteria with LLM judge
        LangChainStringEvaluator(
            "labeled_criteria",
            config={
                "criteria": {
                    "usefulness": "The prediction is useful if it is correct "
                                 "and/or asks a useful followup question."
                },
                "llm": ChatOpenAI(model="gpt-4o")
            },
            prepare_data=prepare_criteria_data
        )
    ],
    description="Evaluating with off-the-shelf LangChain evaluators."
)
```

**LangChain 内置评估器类型**：

- `embedding_distance`：基于嵌入的相似度
- `labeled_criteria`：基于自定义标准的 LLM 评判
- `string_distance`：字符串编辑距离
- `exact_match`：精确匹配

### 评估指标

#### 常用指标

1. **准确率（Accuracy）**：正确预测的比例

```python
def accuracy(outputs: dict, reference_outputs: dict) -> dict:
    pred = outputs["response"]
    expected = reference_outputs["answer"]
    return {
        "key": "accuracy",
        "score": int(expected.lower() == pred.lower())
    }
```

2. **精确率（Precision）**：正类预测的准确性

```python
def precision(outputs: list[dict], reference_outputs: list[dict]) -> dict:
    predictions = [out["response"].lower() for out in outputs]
    expected = [ref["answer"].lower() for ref in reference_outputs]

    tp = sum([p == e for p, e in zip(predictions, expected) if p == "yes"])
    fp = sum([p == "yes" and e == "no" for p, e in zip(predictions, expected)])

    return {"key": "precision", "score": tp / (tp + fp) if tp + fp > 0 else 0.0}
```

3. **召回率（Recall）**：正类被找出的比例

```python
def recall(outputs: list[dict], reference_outputs: list[dict]) -> dict:
    predictions = [out["response"].lower() for out in outputs]
    expected = [ref["answer"].lower() for ref in reference_outputs]

    tp = sum([p == e for p, e in zip(predictions, expected) if p == "yes"])
    fn = sum([p == "no" and e == "yes" for p, e in zip(predictions, expected)])

    return {"key": "recall", "score": tp / (tp + fn) if tp + fn > 0 else 0.0}
```

4. **F1 分数**：精确率和召回率的调和平均

5. **语义相似度**：使用嵌入计算相似度

6. **LLM 评判分数**：使用强大的 LLM 作为评判器

### 收集用户反馈

除了自动评估，LangSmith 还支持收集人类反馈：

```python
from langsmith import trace, traceable, Client

@traceable
def foo(x):
    return {"y": x * 2}

@traceable
def bar(y):
    return {"z": y - 1}

client = Client()

inputs = {"x": 1}
with trace(name="foobar", inputs=inputs) as root_run:
    result = foo(**inputs)
    result = bar(**result)
    root_run.outputs = result
    trace_id = root_run.id
    child_runs = root_run.child_runs

# Provide feedback for a trace (root run)
client.create_feedback(
    key="user_feedback",
    score=1,                    # 1 for thumbs up, 0 for thumbs down
    trace_id=trace_id,
    comment="Great response!"
)

# Provide feedback for a child run
foo_run_id = [run for run in child_runs if run.name == "foo"][0].id
client.create_feedback(
    key="correctness",
    score=0,
    run_id=foo_run_id,
    trace_id=trace_id,
    comment="Incorrect calculation"
)
```

**反馈类型**：

- **Score**：数值评分（如 0-1、1-5 星）
- **Value**：文本或结构化数据
- **Comment**：详细评论
- **Correction**：正确答案或修正

**创建预签名反馈 URL**：

```python
# Create a presigned URL for collecting feedback
token_info = client.create_presigned_feedback_token(
    run_id="run-123",
    feedback_key="user_rating"
)

print(f"Share this URL with users: {token_info['url']}")
# Users can submit feedback without authentication
```

### 代码示例：完整的评估实践

```python
import asyncio
from langsmith import Client
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Step 1: Initialize client
client = Client()

# Step 2: Define the application to evaluate
llm = ChatOpenAI(model="gpt-4")
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Answer the question concisely."),
    ("user", "{question}")
])
chain = prompt | llm | StrOutputParser()

async def predict_async(inputs: dict) -> dict:
    """Async wrapper for the chain"""
    result = await chain.ainvoke({"question": inputs["question"]})
    return {"answer": result}

# Step 3: Create evaluation dataset
dataset_name = "qa-evaluation-set"

try:
    dataset = client.read_dataset(dataset_name=dataset_name)
    print(f"Using existing dataset: {dataset_name}")
except:
    dataset = client.create_dataset(dataset_name=dataset_name)
    print(f"Created new dataset: {dataset_name}")

    # Add examples
    examples = [
        {
            "inputs": {"question": "What is the capital of France?"},
            "outputs": {"answer": "Paris"},
            "metadata": {"category": "geography"}
        },
        {
            "inputs": {"question": "Who wrote Romeo and Juliet?"},
            "outputs": {"answer": "William Shakespeare"},
            "metadata": {"category": "literature"}
        },
        {
            "inputs": {"question": "What is 2+2?"},
            "outputs": {"answer": "4"},
            "metadata": {"category": "math"}
        }
    ]

    client.create_examples(dataset_name=dataset_name, examples=examples)

# Step 4: Define evaluators
def answer_correctness(outputs: dict, reference_outputs: dict) -> dict:
    """Check if answer contains expected content"""
    answer = outputs["answer"].lower()
    expected = reference_outputs["answer"].lower()

    # Simple substring check
    score = 1 if expected in answer else 0

    return {
        "key": "correctness",
        "score": score
    }

async def answer_quality(outputs: dict) -> dict:
    """Use LLM to judge answer quality"""
    judge_llm = ChatOpenAI(model="gpt-4")

    response = await judge_llm.ainvoke([
        {"role": "system", "content": "Rate the quality of the answer from 0 to 1."},
        {"role": "user", "content": f"Answer: {outputs['answer']}"}
    ])

    # Parse score (simplified)
    content = response.content.lower()
    if "0." in content:
        score = float(content.split("0.")[1][:1]) / 10 + 0.0
    else:
        score = 0.5  # Default

    return {
        "key": "quality",
        "score": score
    }

def overall_accuracy(outputs: list[dict], reference_outputs: list[dict]) -> dict:
    """Calculate overall accuracy across all examples"""
    correct = 0
    total = len(outputs)

    for out, ref in zip(outputs, reference_outputs):
        if ref["answer"].lower() in out["answer"].lower():
            correct += 1

    return {
        "key": "overall_accuracy",
        "score": correct / total if total > 0 else 0
    }

# Step 5: Run evaluation
print("Running evaluation...")

results = asyncio.run(
    client.aevaluate(
        predict_async,
        data=dataset_name,
        evaluators=[
            answer_correctness,
            answer_quality
        ],
        summary_evaluators=[
            overall_accuracy
        ],
        experiment_prefix="QA System Evaluation",
        description="Evaluating question answering system with multiple metrics",
        metadata={
            "model": "gpt-4",
            "version": "1.0.0",
            "timestamp": "2025-12-30"
        },
        max_concurrency=5  # Run 5 examples in parallel
    )
)

print(f"\nEvaluation completed!")
print(f"View results at: https://smith.langchain.com/")
```

---

## 最佳实践

### 可观测性最佳实践

#### 1. 合理组织 Projects

```python
import os

# Development
os.environ["LANGSMITH_PROJECT"] = "my-app-dev"

# Staging
os.environ["LANGSMITH_PROJECT"] = "my-app-staging"

# Production
os.environ["LANGSMITH_PROJECT"] = "my-app-prod"
```

**建议**：

- 为不同环境使用不同项目
- 使用描述性的项目名称
- 定期清理旧项目数据

#### 2. 添加有意义的 Metadata 和 Tags

```python
@traceable(
    metadata={
        "version": "2.1.0",
        "user_id": "user-123",
        "session_id": "session-456",
        "experiment": "prompt-v2"
    },
    tags=["production", "premium-user", "critical"]
)
def important_function(input_data):
    pass
```

**建议**：

- 添加版本信息便于 A/B 测试
- 记录用户信息用于调试
- 使用标签分类不同类型的请求

#### 3. 捕获关键性能指标

```python
from langsmith import traceable
import time

@traceable
def expensive_operation(data):
    start = time.time()

    # Perform operation
    result = process_data(data)

    latency = time.time() - start

    # Metadata will be included in trace
    return {
        "result": result,
        "latency_ms": latency * 1000,
        "data_size": len(data)
    }
```

#### 4. 错误处理和日志

```python
from langsmith import traceable

@traceable
def robust_function(input_data):
    try:
        result = risky_operation(input_data)
        return {"status": "success", "result": result}
    except ValueError as e:
        # Error will be captured in trace
        return {
            "status": "error",
            "error_type": "ValueError",
            "error_message": str(e)
        }
    except Exception as e:
        return {
            "status": "error",
            "error_type": type(e).__name__,
            "error_message": str(e)
        }
```

#### 5. 采样策略（高流量场景）

```python
import random
from langsmith import traceable

@traceable
def high_traffic_endpoint(request):
    # Only trace 10% of requests
    if random.random() < 0.1:
        # Full tracing
        return process_request(request)
    else:
        # No tracing for this request
        return process_request(request)
```

### 评估最佳实践

#### 1. 构建高质量数据集

```python
# Good: Diverse, representative examples
examples = [
    # Easy cases
    {"inputs": {"q": "2+2"}, "outputs": {"a": "4"}},

    # Medium cases
    {"inputs": {"q": "sqrt(16)"}, "outputs": {"a": "4"}},

    # Hard cases
    {"inputs": {"q": "solve x^2 + 5x + 6 = 0"}, "outputs": {"a": "x = -2 or x = -3"}},

    # Edge cases
    {"inputs": {"q": "1/0"}, "outputs": {"a": "undefined"}},

    # Ambiguous cases
    {"inputs": {"q": "what is blue?"}, "outputs": {"a": "A color or feeling of sadness"}}
]
```

**建议**：

- 覆盖不同难度级别
- 包含边缘情况
- 定期更新数据集
- 记录数据来源

#### 2. 使用多种评估器

```python
evaluators = [
    # Rule-based
    exact_match_evaluator,

    # Heuristic
    substring_match_evaluator,

    # Similarity-based
    embedding_similarity_evaluator,

    # LLM-based
    llm_judge_evaluator
]
```

**建议**：

- 结合客观和主观指标
- 使用快速评估器进行初筛
- 使用 LLM 评估器进行深度分析
- 定期验证评估器的准确性

#### 3. 版本控制和实验管理

```python
# Version 1
results_v1 = client.evaluate(
    predict_v1,
    data=dataset_name,
    evaluators=evaluators,
    experiment_prefix="System v1.0",
    metadata={"version": "1.0", "prompt": "basic"}
)

# Version 2
results_v2 = client.evaluate(
    predict_v2,
    data=dataset_name,
    evaluators=evaluators,
    experiment_prefix="System v2.0",
    metadata={"version": "2.0", "prompt": "enhanced"}
)

# Compare results in LangSmith UI
```

#### 4. 自动化评估流程

```python
# Example: CI/CD integration
import sys

def run_evaluation_suite():
    results = client.evaluate(
        predict,
        data="regression-test-set",
        evaluators=evaluators,
        experiment_prefix="CI Build",
        metadata={"commit": "abc123"}
    )

    # Check if results meet threshold
    if results["overall_accuracy"] < 0.85:
        print("FAILED: Accuracy below threshold")
        sys.exit(1)
    else:
        print("PASSED: All tests passed")
        sys.exit(0)

if __name__ == "__main__":
    run_evaluation_suite()
```

#### 5. 结合在线和离线评估

```python
# Offline: Pre-deployment testing
offline_results = client.evaluate(
    new_model,
    data="test-set",
    evaluators=evaluators
)

# Online: Monitor production
from langsmith import traceable

@traceable
def production_endpoint(request):
    result = model.predict(request)

    # Collect user feedback
    return {
        "result": result,
        "feedback_url": create_feedback_url(request.id)
    }
```

### 性能优化

#### 1. 异步评估

```python
# Faster evaluation with async
results = asyncio.run(
    client.aevaluate(
        async_predict,
        data=dataset_name,
        evaluators=[async_evaluator],
        max_concurrency=10  # Adjust based on rate limits
    )
)
```

#### 2. 批量处理

```python
# Batch create examples
client.create_examples(
    dataset_name="my-dataset",
    examples=large_example_list  # Process in batches automatically
)
```

#### 3. 缓存结果

```python
# LangSmith automatically caches evaluation results
# Re-running on same dataset with same function will use cache
```

---

## 常见问题（FAQ）

### 可观测性相关

**Q1: LangSmith 会增加应用延迟吗？**

A: 不会。LangSmith SDK 使用异步回调处理器，追踪数据在后台发送，不会阻塞应用执行。

**Q2: 数据存储在哪里？**

A:
- 默认（US 区域）：GCP us-central-1
- EU 区域：可选
- 企业版：可部署在您的 Kubernetes 集群中（AWS/GCP/Azure）

**Q3: 如何关闭追踪？**

A:
```python
import os
os.environ["LANGSMITH_TRACING"] = "false"
# 或者删除环境变量
```

**Q4: 可以追踪非 LangChain 应用吗？**

A: 可以！使用 `@traceable` 装饰器或 `trace` 上下文管理器即可追踪任何 Python 函数。

**Q5: 如何在多线程/多进程环境中使用？**

A: LangSmith SDK 是线程安全的。对于多进程，每个进程应该有自己的 Client 实例。

### 评估相关

**Q6: 如何选择合适的评估指标？**

A: 根据应用类型选择：
- 分类任务：准确率、精确率、召回率、F1
- 生成任务：BLEU、ROUGE、语义相似度、LLM 评判
- 检索任务：MRR、NDCG、Precision@K

**Q7: LLM 评估器的成本如何控制？**

A:
- 使用更便宜的模型（如 GPT-3.5）作为评判器
- 采样评估（不是每个样本都用 LLM 评估）
- 先用规则过滤，再用 LLM 深度评估

**Q8: 如何处理评估器的不确定性？**

A:
- 多次运行实验取平均值
- 使用多个评估器交叉验证
- 结合自动评估和人工审核

**Q9: 数据集应该有多大？**

A:
- 最小：20-50 个代表性样本（快速迭代）
- 推荐：100-500 个样本（全面评估）
- 大规模：1000+ 样本（统计显著性）

**Q10: 如何从生产数据创建数据集？**

A:
```python
# Filter and sample production runs
runs = client.list_runs(
    project_name="production",
    filter='and(eq(status, "success"), gt(feedback.score, 0.8))'
)

# Create dataset from high-quality runs
for run in runs:
    client.create_example_from_run(run, dataset_id=dataset_id)
```

### 故障排查

**Q11: 追踪数据没有显示在 UI 中？**

检查清单：
- ✅ `LANGSMITH_TRACING=true` 已设置
- ✅ `LANGSMITH_API_KEY` 正确
- ✅ 网络连接正常
- ✅ 项目名称正确

**Q12: 评估运行失败？**

常见原因：
- 数据集格式不正确
- 评估函数抛出异常
- API 配额超限
- 网络超时

调试建议：
```python
# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)
```

**Q13: 如何调试评估器？**

```python
# Test evaluator on single example
example = client.read_example(example_id="...")
run = client.read_run(run_id="...")

result = my_evaluator(run.outputs, example.outputs)
print(result)  # Should return {"key": "...", "score": ...}
```

---

## 参考链接

### 官方文档

- [LangSmith 官方网站](https://smith.langchain.com)
- [LangSmith 文档 - 可观测性](https://docs.langchain.com/langsmith/observability)
- [LangSmith 文档 - 评估](https://docs.langchain.com/langsmith/evaluation)
- [LangSmith Python SDK](https://github.com/langchain-ai/langsmith-sdk)

### 教程和指南

- [LangSmith Tracing Deep Dive](https://medium.com/@aviadr1/langsmith-tracing-deep-dive-beyond-the-docs-75016c91f747)
- [Ultimate LangSmith Guide for 2025](https://www.analyticsvidhya.com/blog/2024/07/ultimate-langsmith-guide/)
- [LangSmith Observability for LLM Applications](https://medium.com/@vinodkrane/langsmith-observability-for-llm-applications-ef5aaf6c2e5b)

### 相关工具

- [LangChain](https://python.langchain.com/)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [OpenTelemetry](https://opentelemetry.io/)

### 社区资源

- [LangChain Discord](https://discord.gg/langchain)
- [LangSmith Changelog](https://changelog.langchain.com/)
- [GitHub Discussions](https://github.com/langchain-ai/langchain/discussions)

---

## 总结

LangSmith 的可观测性和评估功能为 LLM 应用开发提供了完整的工具链：

### 可观测性

- 使用 `@traceable` 装饰器轻松追踪函数
- 通过 `wrap_openai/wrap_gemini` 集成主流 LLM
- 在 UI 中可视化追踪树和性能指标
- 使用 Polly AI 助手获取智能洞察
- 设置告警和仪表板监控生产环境

### 评估

- 创建和管理评估数据集
- 定义行级和汇总评估器
- 使用 LLM 作为评判器
- 运行实验并比较结果
- 收集用户反馈持续改进

通过结合这两个功能，您可以构建高质量、可靠的 LLM 应用，并在整个开发生命周期中持续优化。

**下一步**：

1. 注册 LangSmith 账户获取 API Key
2. 在您的项目中配置环境变量
3. 为关键函数添加 `@traceable` 装饰器
4. 创建评估数据集并运行第一个实验
5. 在生产环境中监控应用性能

祝您使用 LangSmith 开发成功！
