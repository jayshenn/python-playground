# LangChain 高级特性

本文档深入探讨 LangChain 的高级特性，帮助你构建生产级的 AI 代理系统。

## 概述

LangChain 1.0 引入了强大的高级特性，使得代理开发更加可靠、结构化和上下文感知。这些特性包括：

- **Middleware（中间件）**：在代理循环的关键点插入自定义逻辑
- **Multi-agent（多代理）**：构建协作的代理系统
- **Context Engineering（上下文工程）**：优化上下文管理和提示工程
- **Human-in-the-Loop（人在环中）**：在关键决策点引入人工审核
- **Guardrails（护栏）**：确保代理输出的安全性和合规性
- **Runtime（运行时）**：依赖注入和配置管理

这些特性基于 LangGraph 的持久化运行时，提供内置的状态持久化、检查点、回滚和人工干预能力。

---

## 1. Middleware（中间件）

### 核心概念

Middleware 是 LangChain 1.0 的核心创新，它提供了一种优雅的方式来扩展代理行为，而无需重写核心逻辑。中间件遵循 Web 服务器中间件的设计模式，按顺序执行，并在返回路径上反向执行。

**中间件的三个关键钩子**：

1. **`before_model`**：在模型调用前执行，可以修改状态或重定向到不同节点
2. **`modify_model_request`**：修改发送给模型的请求（工具、提示、消息列表等）
3. **`after_model`**：在模型调用后执行，可以更新状态或重定向节点

### 内置 Middleware

LangChain 提供了 11 种生产就绪的内置中间件：

#### 1.1 Summarization Middleware

当对话历史接近 token 限制时自动压缩，同时保留最近的消息。

```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, calculator_tool],
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",  # Use cheaper model for summarization
            trigger=("tokens", 4000),  # Trigger when approaching 4k tokens
            keep=("messages", 20),  # Keep last 20 messages intact
        ),
    ],
)
```

#### 1.2 Model Call Limit

限制模型调用次数，防止失控的代理产生过多 API 调用。

```python
from langchain.agents import create_agent
from langchain.agents.middleware import ModelCallLimitMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool],
    middleware=[
        ModelCallLimitMiddleware(
            thread_limit=10,  # Max 10 calls per thread
            run_limit=5,  # Max 5 calls per single run
        ),
    ],
)
```

#### 1.3 Tool Call Limit

控制工具执行次数，支持全局或按工具限制。

```python
from langchain.agents import create_agent
from langchain.agents.middleware import ToolCallLimitMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, write_file_tool, execute_sql_tool],
    middleware=[
        ToolCallLimitMiddleware(
            limits={
                "search": 5,  # Max 5 search calls
                "write_file": 2,  # Max 2 file writes
                "execute_sql": 1,  # Only 1 SQL execution
            },
            on_limit="error",  # Options: 'continue', 'error', 'end'
        ),
    ],
)
```

#### 1.4 Model Fallback

主模型失败时自动切换到备用模型。

```python
from langchain.agents import create_agent
from langchain.agents.middleware import ModelFallbackMiddleware

agent = create_agent(
    model="openai:gpt-4o",  # Primary model
    tools=[search_tool],
    middleware=[
        ModelFallbackMiddleware(
            "openai:gpt-4o-mini",  # First fallback
            "anthropic:claude-sonnet-4-5-20250929",  # Second fallback
        ),
    ],
)

# If gpt-4o fails: tries gpt-4o-mini, then claude-sonnet
result = await agent.invoke({"messages": [{"role": "user", "content": "Hello"}]})
```

#### 1.5 PII Detection

检测和处理敏感信息，支持四种策略。

```python
from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool],
    middleware=[
        # Redact emails in input
        PIIMiddleware(
            "email",
            strategy="redact",  # Options: 'block', 'redact', 'mask', 'hash'
            apply_to_input=True,
        ),
        # Redact phone numbers in output
        PIIMiddleware(
            "phone",
            strategy="redact",
            apply_to_output=True,
        ),
        # Custom regex detector
        PIIMiddleware(
            detector_type="regex",
            pattern=r"\b\d{3}-\d{2}-\d{4}\b",  # SSN pattern
            strategy="mask",
            apply_to_input=True,
            apply_to_output=True,
        ),
    ],
)
```

#### 1.6 LLM Tool Selector

使用 LLM 智能过滤相关工具，减少 token 使用。

```python
from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolSelectorMiddleware

# When you have many tools, only pass relevant ones to the main model
agent = create_agent(
    model="gpt-4o",
    tools=[tool1, tool2, tool3, ..., tool50],  # 50 tools total
    middleware=[
        LLMToolSelectorMiddleware(
            model="gpt-4o-mini",  # Use cheaper model for selection
            max_tools=5,  # Only pass top 5 relevant tools
        ),
    ],
)
```

#### 1.7 Tool Retry & Model Retry

自动重试失败的工具或模型调用，支持指数退避。

```python
from langchain.agents import create_agent
from langchain.agents.middleware import ToolRetryMiddleware, ModelRetryMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[api_call_tool],
    middleware=[
        ToolRetryMiddleware(
            max_retries=3,
            backoff_factor=2.0,  # Exponential backoff
        ),
        ModelRetryMiddleware(
            max_retries=2,
        ),
    ],
)
```

#### 1.8 To-do List

自动为代理提供任务规划能力。

```python
from langchain.agents import create_agent
from langchain.agents.middleware import TodoListMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool],
    middleware=[
        TodoListMiddleware(),  # Adds 'write_todos' tool automatically
    ],
)

# Agent can now plan tasks
result = agent.invoke({
    "messages": [{"role": "user", "content": "Research AI trends and write a report"}]
})
```

### 自定义 Middleware

创建自定义中间件有两种方式：类方式和装饰器方式。

#### 类方式

```python
from typing import Any
from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config
from langgraph.runtime import Runtime

class CustomLoggingMiddleware(AgentMiddleware):
    """Log all model interactions for debugging."""

    def __init__(self, log_file: str):
        super().__init__()
        self.log_file = log_file

    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        """Log before model call."""
        with open(self.log_file, 'a') as f:
            f.write(f"Before model: {state['messages'][-1].content}\n")
        return None

    @hook_config(can_jump_to=["end"])
    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        """Log after model call."""
        if state["messages"]:
            last_msg = state["messages"][-1]
            with open(self.log_file, 'a') as f:
                f.write(f"After model: {last_msg.content}\n")
        return None

# Use the custom middleware
agent = create_agent(
    model="gpt-4o",
    tools=[search_tool],
    middleware=[
        CustomLoggingMiddleware(log_file="agent.log"),
    ],
)
```

#### 装饰器方式

```python
from langchain.agents.middleware import before_model, after_model, AgentState
from langgraph.runtime import Runtime
from typing import Any

@before_model
def log_input(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """Log user input."""
    print(f"User input: {state['messages'][0].content}")
    return None

@after_model
def log_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """Log agent output."""
    if state["messages"]:
        print(f"Agent output: {state['messages'][-1].content}")
    return None

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool],
    middleware=[log_input, log_output],
)
```

### Middleware 使用场景和最佳实践

**使用场景**：
- 日志记录和监控
- 性能分析和调试
- 上下文压缩和管理
- 输入/输出验证
- 错误处理和重试
- 缓存优化

**最佳实践**：

1. **分离关注点**：每个中间件只负责一个特定功能
2. **保持顺序**：中间件执行顺序很重要，考虑依赖关系
3. **避免副作用**：中间件应该是可预测的
4. **合理使用 `jump_to`**：只在必要时跳转节点
5. **性能优化**：避免在中间件中执行重型操作

```python
# Good: Layered middleware with clear separation
agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, send_email_tool],
    middleware=[
        # Layer 1: Input validation (fast, deterministic)
        ContentFilterMiddleware(banned_keywords=["hack"]),

        # Layer 2: PII protection
        PIIMiddleware("email", strategy="redact", apply_to_input=True),

        # Layer 3: Context management
        SummarizationMiddleware(model="gpt-4o-mini", trigger=("tokens", 4000)),

        # Layer 4: Human approval for sensitive actions
        HumanInTheLoopMiddleware(interrupt_on={"send_email": True}),

        # Layer 5: Output validation
        PIIMiddleware("email", strategy="redact", apply_to_output=True),
    ],
)
```

---

## 2. Multi-agent（多代理）

### 核心概念

多代理系统允许构建由多个专门代理组成的协作系统。LangChain 和 LangGraph 支持四种主要的多代理模式：

1. **Router（路由）**：根据输入选择合适的代理
2. **Handoffs（交接）**：代理间传递控制权
3. **Subagents（子代理）**：层次化代理结构
4. **Skills（技能）**：可重用的代理能力

### 2.1 Router（路由）

路由模式使用结构化输出选择最合适的代理来处理请求。

```python
from typing import Literal
from typing_extensions import TypedDict
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph.state import StateGraph, START, END
from langchain.chat_models import init_chat_model

# Define routing schema
class Route(BaseModel):
    step: Literal["technical_support", "sales", "general_inquiry"] = Field(
        None, description="The department to route the inquiry to"
    )

# Initialize model with structured output
llm = init_chat_model("gpt-4o")
router = llm.with_structured_output(Route)

# Define state
class State(TypedDict):
    input: str
    department: str
    output: str

# Define specialized agents
def technical_support_agent(state: State):
    """Handle technical support inquiries."""
    llm = init_chat_model("gpt-4o")
    result = llm.invoke([
        SystemMessage(content="You are a technical support specialist. Help users solve technical problems."),
        HumanMessage(content=state["input"])
    ])
    return {"output": result.content}

def sales_agent(state: State):
    """Handle sales inquiries."""
    llm = init_chat_model("gpt-4o")
    result = llm.invoke([
        SystemMessage(content="You are a sales representative. Help customers with product information and purchases."),
        HumanMessage(content=state["input"])
    ])
    return {"output": result.content}

def general_inquiry_agent(state: State):
    """Handle general inquiries."""
    llm = init_chat_model("gpt-4o")
    result = llm.invoke([
        SystemMessage(content="You are a customer service representative. Handle general inquiries politely."),
        HumanMessage(content=state["input"])
    ])
    return {"output": result.content}

def router_node(state: State):
    """Route the inquiry to the appropriate department."""
    decision = router.invoke([
        SystemMessage(content="Route the customer inquiry to technical_support, sales, or general_inquiry based on the content."),
        HumanMessage(content=state["input"]),
    ])
    return {"department": decision.step}

# Conditional edge function
def route_decision(state: State):
    """Return the node name to visit next."""
    return state["department"]

# Build the routing graph
router_builder = StateGraph(State)

# Add nodes
router_builder.add_node("router", router_node)
router_builder.add_node("technical_support", technical_support_agent)
router_builder.add_node("sales", sales_agent)
router_builder.add_node("general_inquiry", general_inquiry_agent)

# Add edges
router_builder.add_edge(START, "router")
router_builder.add_conditional_edges(
    "router",
    route_decision,
    {
        "technical_support": "technical_support",
        "sales": "sales",
        "general_inquiry": "general_inquiry",
    },
)
router_builder.add_edge("technical_support", END)
router_builder.add_edge("sales", END)
router_builder.add_edge("general_inquiry", END)

# Compile the graph
router_graph = router_builder.compile()

# Use the router
result = router_graph.invoke({
    "input": "My software keeps crashing when I try to save files"
})
print(result["output"])
```

### 2.2 Handoffs（交接）

交接模式允许当前代理将控制权转移给另一个代理，实现去中心化的控制流。

```python
from langgraph.graph.state import StateGraph, START, END
from langgraph.types import Command
from typing_extensions import TypedDict, Literal

class State(TypedDict):
    messages: list
    current_agent: str
    handoff_reason: str

def customer_service_agent(state: State) -> Command[Literal["technical_agent", "billing_agent", "end"]]:
    """First-line customer service agent."""
    messages = state["messages"]
    last_message = messages[-1]["content"].lower()

    # Analyze the request and decide whether to handoff
    if "password" in last_message or "login" in last_message:
        return Command(
            update={
                "messages": messages + [{
                    "role": "assistant",
                    "content": "I'll transfer you to our technical support team for login issues."
                }],
                "current_agent": "technical_agent",
                "handoff_reason": "Technical issue - login/password"
            },
            goto="technical_agent"
        )
    elif "bill" in last_message or "payment" in last_message or "refund" in last_message:
        return Command(
            update={
                "messages": messages + [{
                    "role": "assistant",
                    "content": "Let me connect you with our billing department."
                }],
                "current_agent": "billing_agent",
                "handoff_reason": "Billing inquiry"
            },
            goto="billing_agent"
        )
    else:
        # Handle general inquiry directly
        return Command(
            update={
                "messages": messages + [{
                    "role": "assistant",
                    "content": "I can help you with that! What would you like to know?"
                }],
                "current_agent": "customer_service",
            },
            goto="end"
        )

def technical_agent(state: State) -> Command[Literal["customer_service_agent", "end"]]:
    """Specialized technical support agent."""
    messages = state["messages"]

    # Simulate technical support
    response = "I've reset your password. Check your email for the new temporary password."

    # Can handoff back to customer service or end
    return Command(
        update={
            "messages": messages + [{
                "role": "assistant",
                "content": response
            }],
        },
        goto="end"
    )

def billing_agent(state: State) -> Command[Literal["customer_service_agent", "end"]]:
    """Specialized billing agent."""
    messages = state["messages"]

    # Simulate billing support
    response = "I've reviewed your account. Your next billing date is in 5 days."

    return Command(
        update={
            "messages": messages + [{
                "role": "assistant",
                "content": response
            }],
        },
        goto="end"
    )

# Build the handoff graph
handoff_builder = StateGraph(State)

handoff_builder.add_node("customer_service_agent", customer_service_agent)
handoff_builder.add_node("technical_agent", technical_agent)
handoff_builder.add_node("billing_agent", billing_agent)

handoff_builder.add_edge(START, "customer_service_agent")

# Compile
handoff_graph = handoff_builder.compile()

# Test handoffs
result = handoff_graph.invoke({
    "messages": [{"role": "user", "content": "I forgot my password"}],
    "current_agent": "customer_service",
    "handoff_reason": ""
})

print(f"Final agent: {result['current_agent']}")
print(f"Messages: {result['messages']}")
```

**何时使用 Command vs Conditional Edges？**

- 使用 `Command`：当需要**同时**更新状态**和**路由到不同节点时（如多代理交接）
- 使用条件边：仅需要条件路由而不更新状态时

### 2.3 Subagents（子代理）

子代理允许构建层次化的代理结构，父图可以包含子图作为节点。

```python
from typing_extensions import TypedDict
from langgraph.graph.state import StateGraph, START, END

# ===== Define Subgraph =====
class SubgraphState(TypedDict):
    query: str  # Shared with parent
    search_results: str  # Private to subgraph
    summary: str  # Private to subgraph

def search_node(state: SubgraphState):
    """Search for information."""
    query = state["query"]
    # Simulate search
    results = f"Search results for '{query}': [Result 1, Result 2, Result 3]"
    return {"search_results": results}

def summarize_node(state: SubgraphState):
    """Summarize search results."""
    results = state["search_results"]
    summary = f"Summary: Found 3 relevant results for the query."
    return {"summary": summary, "query": state["query"] + f" | {summary}"}

# Build subgraph
subgraph_builder = StateGraph(SubgraphState)
subgraph_builder.add_node("search", search_node)
subgraph_builder.add_node("summarize", summarize_node)
subgraph_builder.add_edge(START, "search")
subgraph_builder.add_edge("search", "summarize")
subgraph = subgraph_builder.compile()

# ===== Define Parent Graph =====
class ParentState(TypedDict):
    query: str  # Shared with subgraph
    final_answer: str

def prepare_query_node(state: ParentState):
    """Prepare the query."""
    return {"query": f"Prepared: {state['query']}"}

def answer_node(state: ParentState):
    """Generate final answer based on subgraph results."""
    # The subgraph updated the query field with summary
    return {"final_answer": f"Final answer based on: {state['query']}"}

# Build parent graph
parent_builder = StateGraph(ParentState)
parent_builder.add_node("prepare", prepare_query_node)
parent_builder.add_node("research", subgraph)  # Subgraph as a node!
parent_builder.add_node("answer", answer_node)

parent_builder.add_edge(START, "prepare")
parent_builder.add_edge("prepare", "research")
parent_builder.add_edge("research", "answer")
parent_builder.add_edge("answer", END)

parent_graph = parent_builder.compile()

# Use the hierarchical system
result = parent_graph.invoke({"query": "What is LangChain?"})
print(result["final_answer"])
```

**子代理导航到父图**：

子图可以使用 `Command.PARENT` 导航到父图中的节点。

```python
from langgraph.types import Command
from typing import Literal

def subgraph_node(state: State) -> Command[Literal["other_subgraph"]]:
    """Navigate to a node in parent graph."""
    return Command(
        update={"data": "processed"},
        goto="other_subgraph",  # Node in parent graph
        graph=Command.PARENT  # Navigate to parent
    )
```

### 2.4 Skills（技能）

技能是可重用的代理能力，可以在多个代理之间共享。

```python
from langchain.tools import tool
from langchain.agents import create_agent

# Define reusable skills as tools
@tool
def web_search_skill(query: str) -> str:
    """Search the web for information."""
    # Implement web search logic
    return f"Search results for: {query}"

@tool
def data_analysis_skill(data: str) -> str:
    """Analyze data and provide insights."""
    # Implement data analysis logic
    return f"Analysis of: {data}"

@tool
def report_writing_skill(content: str) -> str:
    """Write a professional report."""
    # Implement report writing logic
    return f"Report: {content}"

# Create specialized agents with different skill combinations
research_agent = create_agent(
    model="gpt-4o",
    tools=[web_search_skill, data_analysis_skill],
)

writer_agent = create_agent(
    model="gpt-4o",
    tools=[report_writing_skill, data_analysis_skill],
)

# Skills can be reused across multiple agents
general_agent = create_agent(
    model="gpt-4o",
    tools=[web_search_skill, data_analysis_skill, report_writing_skill],
)
```

### 多代理最佳实践

1. **明确职责**：每个代理应该有清晰的专业领域
2. **合理路由**：使用适当的路由策略（集中式 vs 去中心式）
3. **状态管理**：清晰定义共享状态和私有状态
4. **错误处理**：考虑代理失败时的回退策略
5. **性能优化**：避免不必要的代理切换

```python
# Good: Clear agent specialization
agents = {
    "data_collector": create_agent(model="gpt-4o-mini", tools=[web_search, api_call]),
    "data_analyzer": create_agent(model="gpt-4o", tools=[analyze_data, visualize]),
    "report_writer": create_agent(model="gpt-4o", tools=[write_report, format_document]),
}

# Bad: Overlapping responsibilities
agents = {
    "agent1": create_agent(tools=[search, analyze, write]),  # Too many responsibilities
    "agent2": create_agent(tools=[search, analyze, write]),  # Duplicate capabilities
}
```

---

## 3. Context Engineering（上下文工程）

### 核心概念

上下文工程是优化提示和上下文管理的艺术，包括：
- 上下文窗口优化
- 提示模板管理
- Few-shot 示例管理
- 动态上下文适配

### 3.1 上下文管理策略

#### 使用 Summarization Middleware

```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool],
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",  # Cheaper model for summarization
            trigger=("tokens", 3000),  # Summarize when approaching 3k tokens
            keep=("messages", 15),  # Keep last 15 messages verbatim
        ),
    ],
)

# Long conversation will be automatically summarized
for i in range(100):
    result = agent.invoke({
        "messages": [{"role": "user", "content": f"Question {i}: Tell me about AI"}]
    })
```

#### Context Editing Middleware

清除旧的工具输出以节省 token。

```python
from langchain.agents import create_agent
from langchain.agents.middleware import ContextEditingMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, api_tool],
    middleware=[
        ContextEditingMiddleware(
            trigger=("tokens", 4000),
            keep_tool_outputs=5,  # Only keep last 5 tool outputs
        ),
    ],
)
```

### 3.2 动态提示工程

#### 基于上下文的系统提示

```python
from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dataclass
class Context:
    user_role: str
    deployment_env: str
    user_expertise: str

@dynamic_prompt
def context_aware_prompt(request: ModelRequest) -> str:
    """Generate system prompt based on runtime context."""
    context = request.runtime.context

    base = "You are a helpful AI assistant."

    # Adapt based on user role
    if context.user_role == "admin":
        base += "\nYou have admin access. You can perform all operations."
    elif context.user_role == "developer":
        base += "\nYou can access development tools and APIs."
    elif context.user_role == "viewer":
        base += "\nYou have read-only access. Guide users to read operations only."

    # Adapt based on environment
    if context.deployment_env == "production":
        base += "\nBe extra careful with any data modifications."
        base += "\nAlways confirm destructive operations."
    elif context.deployment_env == "development":
        base += "\nYou can freely experiment in this environment."

    # Adapt based on expertise
    if context.user_expertise == "beginner":
        base += "\nProvide detailed explanations and step-by-step guidance."
    elif context.user_expertise == "expert":
        base += "\nBe concise and technical. Skip basic explanations."

    return base

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, code_tool],
    middleware=[context_aware_prompt],
    context_schema=Context,
)

# Invoke with different contexts
result = agent.invoke(
    {"messages": [{"role": "user", "content": "How do I deploy this app?"}]},
    context=Context(
        user_role="developer",
        deployment_env="production",
        user_expertise="expert"
    )
)
```

### 3.3 Few-shot 示例管理

```python
from langchain_core.prompts import (
    ChatPromptTemplate,
    FewShotChatMessagePromptTemplate,
)
from langchain.chat_models import init_chat_model

# Define few-shot examples
examples = [
    {
        "input": "What is 2+2?",
        "output": "2+2 equals 4."
    },
    {
        "input": "Calculate 10*5",
        "output": "10*5 equals 50."
    },
    {
        "input": "What is the square root of 16?",
        "output": "The square root of 16 is 4."
    },
]

# Create example prompt template
example_prompt = ChatPromptTemplate.from_messages([
    ("human", "{input}"),
    ("ai", "{output}"),
])

# Create few-shot prompt
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
)

# Create final prompt with system message and few-shot examples
final_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a math tutor. Answer math questions clearly and concisely."),
    few_shot_prompt,
    ("human", "{input}"),
])

# Use with LLM
llm = init_chat_model("gpt-4o")
chain = final_prompt | llm

result = chain.invoke({"input": "What is 15 + 27?"})
print(result.content)
```

#### 动态 Few-shot 选择

```python
from langchain_core.prompts import FewShotChatMessagePromptTemplate
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# Define a larger set of examples
examples = [
    {"input": "2+2", "output": "4"},
    {"input": "2*3", "output": "6"},
    {"input": "sqrt(16)", "output": "4"},
    {"input": "10^2", "output": "100"},
    {"input": "sin(0)", "output": "0"},
    {"input": "cos(0)", "output": "1"},
    {"input": "log(1)", "output": "0"},
    {"input": "5!", "output": "120"},
]

# Create semantic similarity selector
example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,
    OpenAIEmbeddings(),
    FAISS,
    k=3,  # Select top 3 most similar examples
)

# Create few-shot prompt with selector
example_prompt = ChatPromptTemplate.from_messages([
    ("human", "{input}"),
    ("ai", "{output}"),
])

few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
)

final_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a math expert."),
    few_shot_prompt,
    ("human", "{input}"),
])

# The selector will pick the most relevant examples based on input similarity
chain = final_prompt | llm
result = chain.invoke({"input": "What is the logarithm of 10?"})
```

### 3.4 上下文窗口优化技巧

```python
from langchain.agents import create_agent
from langchain.agents.middleware import (
    SummarizationMiddleware,
    ContextEditingMiddleware,
)

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, calculator_tool],
    middleware=[
        # Strategy 1: Summarize old messages
        SummarizationMiddleware(
            model="gpt-4o-mini",
            trigger=("tokens", 3000),
            keep=("messages", 10),
        ),

        # Strategy 2: Clear old tool outputs
        ContextEditingMiddleware(
            trigger=("tokens", 4000),
            keep_tool_outputs=5,
        ),
    ],
)
```

### Context Engineering 最佳实践

1. **监控 token 使用**：始终跟踪上下文长度
2. **分层总结**：对不同类型的信息使用不同的总结策略
3. **保留关键信息**：确保重要的上下文不被删除
4. **动态适配**：根据任务复杂度调整上下文策略
5. **测试提示**：A/B 测试不同的提示策略

---

## 4. Human-in-the-Loop（人在环中）

### 核心概念

HITL 允许在代理执行敏感操作前引入人工审核，是构建可信 AI 系统的关键。

### 4.1 基本 HITL 工作流

```python
from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import Command

# Define tools
from langchain.tools import tool

@tool
def search_tool(query: str) -> str:
    """Search the web - safe operation."""
    return f"Search results for: {query}"

@tool
def send_email_tool(to: str, subject: str, body: str) -> str:
    """Send email - requires approval."""
    return f"Email sent to {to}"

@tool
def delete_database_tool(table: str) -> str:
    """Delete database table - requires approval."""
    return f"Deleted table: {table}"

# Create agent with HITL
agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, send_email_tool, delete_database_tool],
    middleware=[
        HumanInTheLoopMiddleware(
            interrupt_on={
                # Require approval for sensitive operations
                "send_email": True,
                "delete_database": True,
                # Auto-approve safe operations
                "search": False,
            },
            description_prefix="Action pending approval",
        ),
    ],
    # HITL requires checkpointer for state persistence
    checkpointer=InMemorySaver(),
)

# Must use thread ID for persistence across interrupts
config = {"configurable": {"thread_id": "conversation_123"}}

# Step 1: Agent pauses and waits for approval
result = agent.invoke(
    {"messages": [{"role": "user", "content": "Send email to team@company.com about the meeting"}]},
    config=config
)

print("Agent is waiting for approval...")
print(f"Proposed action: {result}")

# Step 2: Human reviews and approves
result = agent.invoke(
    Command(resume={"decisions": [{"type": "approve"}]}),
    config=config  # Same thread ID to resume
)

print(f"Final result: {result}")
```

### 4.2 不同的审批策略

```python
from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
    model="gpt-4o",
    tools=[write_file_tool, execute_sql_tool, read_data_tool],
    middleware=[
        HumanInTheLoopMiddleware(
            interrupt_on={
                # Full control: approve, reject, or edit
                "write_file": True,

                # Limited control: only approve or reject (no editing)
                "execute_sql": {"allowed_decisions": ["approve", "reject"]},

                # Auto-approve (no interrupt)
                "read_data": False,
            },
            description_prefix="Tool execution pending approval",
        ),
    ],
    checkpointer=InMemorySaver(),
)

config = {"configurable": {"thread_id": "thread_456"}}

# Invoke agent
result = agent.invoke(
    {"messages": [{"role": "user", "content": "Write config.json with production settings"}]},
    config=config
)

# Option 1: Approve
result = agent.invoke(
    Command(resume={"decisions": [{"type": "approve"}]}),
    config=config
)

# Option 2: Reject
result = agent.invoke(
    Command(resume={"decisions": [{"type": "reject"}]}),
    config=config
)

# Option 3: Edit (if allowed)
result = agent.invoke(
    Command(resume={"decisions": [{
        "type": "edit",
        "args": {"path": "config.dev.json", "content": "development settings"}
    }]}),
    config=config
)
```

### 4.3 自定义 HITL 中间件

```python
from typing import Any
from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config
from langgraph.runtime import Runtime

class CustomApprovalMiddleware(AgentMiddleware):
    """Custom HITL with approval logic."""

    def __init__(self, approval_callback):
        super().__init__()
        self.approval_callback = approval_callback

    @hook_config(can_jump_to=["end"])
    def before_tool(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        """Interrupt before tool execution for approval."""
        if not state.get("tool_calls"):
            return None

        for tool_call in state["tool_calls"]:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]

            # Call custom approval logic
            approved = self.approval_callback(tool_name, tool_args)

            if not approved:
                return {
                    "messages": [{
                        "role": "assistant",
                        "content": f"Tool call to {tool_name} was not approved."
                    }],
                    "jump_to": "end"
                }

        return None

# Custom approval function
def my_approval_logic(tool_name: str, tool_args: dict) -> bool:
    """Custom approval logic - could integrate with external system."""
    print(f"Approve {tool_name} with args {tool_args}? (y/n)")
    response = input().strip().lower()
    return response == 'y'

agent = create_agent(
    model="gpt-4o",
    tools=[send_email_tool, delete_tool],
    middleware=[
        CustomApprovalMiddleware(approval_callback=my_approval_logic),
    ],
)
```

### 4.4 HITL 使用场景

**适合使用 HITL 的场景**：
- 金融交易和转账
- 生产数据修改
- 外部通信（邮件、消息）
- 删除操作
- 法律或合规相关决策
- 高成本操作

**不适合使用 HITL 的场景**：
- 读取操作
- 搜索和查询
- 低风险的数据分析
- 高频率操作

### HITL 最佳实践

1. **明确审批范围**：清晰定义哪些操作需要审批
2. **提供上下文**：给审批者足够的信息做决策
3. **设置超时**：避免无限期等待
4. **记录决策**：保存所有审批记录用于审计
5. **优雅降级**：审批失败时有合理的回退策略

```python
# Good: Clear approval policies
agent = create_agent(
    model="gpt-4o",
    tools=[read_tool, write_tool, delete_tool, send_email_tool],
    middleware=[
        HumanInTheLoopMiddleware(
            interrupt_on={
                "read_tool": False,  # Safe, auto-approve
                "write_tool": True,  # Needs review
                "delete_tool": True,  # Needs review
                "send_email": True,  # Needs review
            },
            description_prefix="[APPROVAL REQUIRED]",
        ),
    ],
    checkpointer=InMemorySaver(),
)
```

---

## 5. Guardrails（护栏）

### 核心概念

Guardrails 是验证和过滤内容的安全机制，在代理执行的关键点实施，包括：
- 输入验证
- 输出过滤
- 内容审核
- 合规性检查

### 5.1 Before-Agent Guardrails（输入验证）

在代理处理前验证和过滤输入。

```python
from typing import Any
from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config
from langgraph.runtime import Runtime

class ContentFilterMiddleware(AgentMiddleware):
    """Deterministic guardrail: Block requests with banned keywords."""

    def __init__(self, banned_keywords: list[str]):
        super().__init__()
        self.banned_keywords = [kw.lower() for kw in banned_keywords]

    @hook_config(can_jump_to=["end"])
    def before_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        """Check input for banned content."""
        if not state["messages"]:
            return None

        first_message = state["messages"][0]
        if first_message.type != "human":
            return None

        content = first_message.content.lower()

        # Check for banned keywords
        for keyword in self.banned_keywords:
            if keyword in content:
                return {
                    "messages": [{
                        "role": "assistant",
                        "content": "I cannot process requests containing inappropriate content. Please rephrase your request."
                    }],
                    "jump_to": "end"  # Short-circuit execution
                }

        return None

# Use the guardrail
from langchain.agents import create_agent

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, calculator_tool],
    middleware=[
        ContentFilterMiddleware(
            banned_keywords=["hack", "exploit", "malware", "illegal"]
        ),
    ],
)

# This will be blocked
result = agent.invoke({
    "messages": [{"role": "user", "content": "How do I hack into a system?"}]
})
# Output: "I cannot process requests containing inappropriate content..."
```

### 5.2 After-Agent Guardrails（输出验证）

验证代理的最终输出。

```python
from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config
from langgraph.runtime import Runtime
from langchain.messages import AIMessage
from langchain.chat_models import init_chat_model
from typing import Any

class SafetyGuardrailMiddleware(AgentMiddleware):
    """Model-based guardrail: Use LLM to evaluate response safety."""

    def __init__(self):
        super().__init__()
        self.safety_model = init_chat_model("gpt-4o-mini")

    @hook_config(can_jump_to=["end"])
    def after_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        """Evaluate response safety using a model."""
        if not state["messages"]:
            return None

        last_message = state["messages"][-1]
        if not isinstance(last_message, AIMessage):
            return None

        # Use safety model to evaluate
        safety_prompt = f"""Evaluate if this response is safe and appropriate.
        Consider:
        - Does it contain harmful instructions?
        - Does it violate ethical guidelines?
        - Does it contain sensitive information?

        Respond with only 'SAFE' or 'UNSAFE'.

        Response: {last_message.content}"""

        result = self.safety_model.invoke([{
            "role": "user",
            "content": safety_prompt
        }])

        if "UNSAFE" in result.content.upper():
            # Replace unsafe content
            last_message.content = "I cannot provide that response. Please rephrase your request."

        return None

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool],
    middleware=[SafetyGuardrailMiddleware()],
)
```

### 5.3 PII Protection（隐私保护）

```python
from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, send_email_tool],
    middleware=[
        # Redact emails in input
        PIIMiddleware(
            "email",
            strategy="redact",  # Options: 'block', 'redact', 'mask', 'hash'
            apply_to_input=True,
        ),

        # Redact phone numbers in both input and output
        PIIMiddleware(
            "phone",
            strategy="mask",  # Replace with ***-***-1234
            apply_to_input=True,
            apply_to_output=True,
        ),

        # Custom regex for SSN
        PIIMiddleware(
            detector_type="regex",
            pattern=r"\b\d{3}-\d{2}-\d{4}\b",  # SSN pattern
            strategy="hash",  # One-way hash
            apply_to_input=True,
            apply_to_output=True,
        ),

        # Block if credit card detected
        PIIMiddleware(
            detector_type="regex",
            pattern=r"\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b",  # Credit card
            strategy="block",  # Reject entire request
            apply_to_input=True,
        ),
    ],
)

# Input with PII will be sanitized
result = agent.invoke({
    "messages": [{
        "role": "user",
        "content": "Send email to john.doe@example.com and call 555-123-4567"
    }]
})
```

### 5.4 分层 Guardrails

组合多个 guardrails 创建深度防御。

```python
from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware, HumanInTheLoopMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, send_email_tool, database_tool],
    middleware=[
        # Layer 1: Deterministic input filter (fastest)
        ContentFilterMiddleware(banned_keywords=["hack", "exploit"]),

        # Layer 2: PII protection (before agent)
        PIIMiddleware("email", strategy="redact", apply_to_input=True),
        PIIMiddleware("phone", strategy="redact", apply_to_input=True),

        # Layer 3: Human approval for sensitive tools
        HumanInTheLoopMiddleware(interrupt_on={
            "send_email": True,
            "database_tool": True,
        }),

        # Layer 4: PII protection (after agent)
        PIIMiddleware("email", strategy="redact", apply_to_output=True),
        PIIMiddleware("phone", strategy="redact", apply_to_output=True),

        # Layer 5: Model-based safety check (final layer)
        SafetyGuardrailMiddleware(),
    ],
)
```

### 5.5 合规性检查

```python
from langchain.agents.middleware import before_agent, AgentState
from langgraph.runtime import Runtime
from typing import Any

@before_agent
def compliance_check(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """Check for compliance violations."""
    if not state["messages"]:
        return None

    content = state["messages"][0].content

    # Check against compliance rules
    violations = []

    # Example: GDPR - check for personal data requests
    if "delete my data" in content.lower():
        # Ensure user is authenticated
        if not runtime.context.get("user_authenticated"):
            violations.append("User must be authenticated for data deletion requests")

    # Example: Financial regulations
    if "investment advice" in content.lower():
        violations.append("System is not licensed to provide investment advice")

    if violations:
        return {
            "messages": [{
                "role": "assistant",
                "content": f"Compliance violation: {', '.join(violations)}"
            }],
            "jump_to": "end"
        }

    return None

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool],
    middleware=[compliance_check],
)
```

### Guardrails 最佳实践

1. **深度防御**：使用多层 guardrails
2. **快速失败**：在处理早期阶段检测问题
3. **清晰反馈**：告诉用户为什么请求被拒绝
4. **定期审计**：检查 guardrails 的有效性
5. **平衡性能**：避免过度检查影响响应时间

```python
# Good: Layered approach with fast-fail
agent = create_agent(
    model="gpt-4o",
    tools=[tools],
    middleware=[
        # Fast deterministic checks first
        ContentFilterMiddleware(...),  # ~1ms
        PIIMiddleware(...),  # ~5ms

        # Human review for edge cases
        HumanInTheLoopMiddleware(...),  # Variable

        # Expensive model-based checks last
        SafetyGuardrailMiddleware(),  # ~500ms
    ],
)

# Bad: Expensive checks first
agent = create_agent(
    model="gpt-4o",
    tools=[tools],
    middleware=[
        SafetyGuardrailMiddleware(),  # Slow, runs even for obviously bad input
        ContentFilterMiddleware(...),  # Should be first
    ],
)
```

---

## 6. Runtime（运行时）

### 核心概念

LangChain 的 `create_agent` 基于 LangGraph 的运行时构建。Runtime 对象封装了代理调用的关键信息：

- **Context**：静态数据（用户 ID、数据库连接、配置等）
- **Store**：长期记忆（BaseStore 实例）
- **Stream writer**：通过 "custom" 流模式传输信息

### 6.1 依赖注入

Runtime Context 提供依赖注入能力，使工具更可测试、可重用和灵活。

#### 在工具中使用 Runtime Context

```python
from dataclasses import dataclass
from langchain.tools import tool, ToolRuntime
from langchain.agents import create_agent

@dataclass
class Context:
    user_id: str
    api_key: str
    db_connection: str
    environment: str

@tool
def fetch_user_data(
    query: str,
    runtime: ToolRuntime[Context]
) -> str:
    """Fetch data using Runtime Context configuration."""
    # Access context via runtime
    user_id = runtime.context.user_id
    api_key = runtime.context.api_key
    db_connection = runtime.context.db_connection

    # Use configuration to fetch data
    print(f"Fetching data for user {user_id} from {db_connection}")
    # results = perform_database_query(db_connection, query, api_key)

    return f"Found results for user {user_id}"

@tool
def send_notification(
    message: str,
    runtime: ToolRuntime[Context]
) -> str:
    """Send notification with environment-specific settings."""
    env = runtime.context.environment

    if env == "production":
        # Send real notification
        return f"Notification sent: {message}"
    else:
        # In dev, just log
        return f"[DEV] Would send: {message}"

agent = create_agent(
    model="gpt-4o",
    tools=[fetch_user_data, send_notification],
    context_schema=Context,
)

# Invoke with runtime context
result = agent.invoke(
    {"messages": [{"role": "user", "content": "Get my data"}]},
    context=Context(
        user_id="user_123",
        api_key="sk-...",
        db_connection="postgresql://...",
        environment="production"
    )
)
```

### 6.2 在 Middleware 中使用 Runtime Context

#### 基于上下文选择模型

```python
from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable

@dataclass
class Context:
    cost_tier: str
    environment: str

# Initialize models once
premium_model = init_chat_model("claude-sonnet-4-5-20250929")
standard_model = init_chat_model("gpt-4o")
budget_model = init_chat_model("gpt-4o-mini")

@wrap_model_call
def context_based_model(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse]
) -> ModelResponse:
    """Select model based on Runtime Context."""
    # Read from Runtime Context
    cost_tier = request.runtime.context.cost_tier
    environment = request.runtime.context.environment

    # Select model based on context
    if environment == "production" and cost_tier == "premium":
        model = premium_model
    elif cost_tier == "budget":
        model = budget_model
    else:
        model = standard_model

    # Override the model
    request = request.override(model=model)

    return handler(request)

agent = create_agent(
    model="gpt-4o",  # Default model
    tools=[search_tool],
    middleware=[context_based_model],
    context_schema=Context,
)

# Premium user gets best model
result = agent.invoke(
    {"messages": [{"role": "user", "content": "Analyze this data"}]},
    context=Context(cost_tier="premium", environment="production")
)
```

#### 基于上下文的动态系统提示

```python
from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dataclass
class Context:
    user_role: str
    deployment_env: str
    company_name: str
    industry: str

@dynamic_prompt
def context_aware_system_prompt(request: ModelRequest) -> str:
    """Generate system prompt based on runtime context."""
    ctx = request.runtime.context

    prompt = f"You are an AI assistant for {ctx.company_name} in the {ctx.industry} industry."

    # Role-based instructions
    if ctx.user_role == "admin":
        prompt += "\nYou have admin access. You can perform all operations."
    elif ctx.user_role == "developer":
        prompt += "\nYou can access development tools and technical documentation."
    elif ctx.user_role == "customer":
        prompt += "\nYou can help with product questions and support issues."

    # Environment-based warnings
    if ctx.deployment_env == "production":
        prompt += "\n⚠️ Production environment - be extra careful with data modifications."

    return prompt

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, code_tool],
    middleware=[context_aware_system_prompt],
    context_schema=Context,
)

result = agent.invoke(
    {"messages": [{"role": "user", "content": "Help me deploy the new feature"}]},
    context=Context(
        user_role="developer",
        deployment_env="production",
        company_name="Acme Corp",
        industry="FinTech"
    )
)
```

### 6.3 资源管理

```python
from dataclasses import dataclass
from contextlib import contextmanager
import psycopg2

@dataclass
class Context:
    db_config: dict

@contextmanager
def get_db_connection(db_config: dict):
    """Context manager for database connections."""
    conn = psycopg2.connect(**db_config)
    try:
        yield conn
    finally:
        conn.close()

@tool
def query_database(
    sql: str,
    runtime: ToolRuntime[Context]
) -> str:
    """Execute SQL query with proper resource management."""
    with get_db_connection(runtime.context.db_config) as conn:
        cursor = conn.cursor()
        cursor.execute(sql)
        results = cursor.fetchall()
        return str(results)

agent = create_agent(
    model="gpt-4o",
    tools=[query_database],
    context_schema=Context,
)

result = agent.invoke(
    {"messages": [{"role": "user", "content": "Show all users"}]},
    context=Context(
        db_config={
            "host": "localhost",
            "database": "mydb",
            "user": "admin",
            "password": "secret"
        }
    )
)
```

### 6.4 环境变量处理

```python
import os
from dataclasses import dataclass
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

@dataclass
class Context:
    openai_api_key: str
    tavily_api_key: str
    environment: str
    debug_mode: bool

    @classmethod
    def from_env(cls):
        """Create context from environment variables."""
        return cls(
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            tavily_api_key=os.getenv("TAVILY_API_KEY"),
            environment=os.getenv("ENVIRONMENT", "development"),
            debug_mode=os.getenv("DEBUG", "false").lower() == "true",
        )

@tool
def web_search(
    query: str,
    runtime: ToolRuntime[Context]
) -> str:
    """Search the web using Tavily."""
    api_key = runtime.context.tavily_api_key
    # Use api_key to make request
    return f"Search results for: {query}"

# Create context from environment
context = Context.from_env()

agent = create_agent(
    model="gpt-4o",
    tools=[web_search],
    context_schema=Context,
)

result = agent.invoke(
    {"messages": [{"role": "user", "content": "Search for AI news"}]},
    context=context
)
```

### 6.5 配置管理

```python
from dataclasses import dataclass
from typing import Dict, Any
import json

@dataclass
class AppConfig:
    """Application configuration."""
    model_settings: Dict[str, Any]
    tool_settings: Dict[str, Any]
    middleware_settings: Dict[str, Any]

    @classmethod
    def from_file(cls, path: str):
        """Load configuration from JSON file."""
        with open(path, 'r') as f:
            config = json.load(f)
        return cls(**config)

@dataclass
class Context:
    config: AppConfig
    user_id: str

@tool
def configured_search(
    query: str,
    runtime: ToolRuntime[Context]
) -> str:
    """Search with configuration."""
    config = runtime.context.config.tool_settings.get("search", {})
    max_results = config.get("max_results", 10)

    return f"Found {max_results} results for: {query}"

# Load configuration
config = AppConfig.from_file("config.json")

agent = create_agent(
    model="gpt-4o",
    tools=[configured_search],
    context_schema=Context,
)

result = agent.invoke(
    {"messages": [{"role": "user", "content": "Search AI trends"}]},
    context=Context(config=config, user_id="user_123")
)
```

### Runtime 最佳实践

1. **使用类型化 Context**：使用 dataclass 定义 Context schema
2. **依赖注入优于全局变量**：通过 Runtime 传递依赖
3. **环境感知**：根据环境（dev/staging/prod）调整行为
4. **配置外部化**：将配置与代码分离
5. **资源清理**：正确管理连接和资源生命周期

```python
# Good: Type-safe, injected dependencies
@dataclass
class Context:
    db: DatabaseConnection
    cache: CacheClient
    logger: Logger

@tool
def my_tool(query: str, runtime: ToolRuntime[Context]) -> str:
    runtime.context.logger.info(f"Executing: {query}")
    results = runtime.context.db.query(query)
    runtime.context.cache.set(query, results)
    return results

# Bad: Global variables, hard to test
DB = connect_database()  # Global
CACHE = connect_cache()  # Global

@tool
def my_tool(query: str) -> str:
    results = DB.query(query)  # Hard to mock
    CACHE.set(query, results)
    return results
```

---

## 常见问题（FAQ）

### Q1: Middleware 的执行顺序是什么？

**A:** Middleware 按照在 `middleware` 列表中的顺序依次执行：
- `before_model` 钩子：按顺序执行（1, 2, 3...）
- `modify_model_request` 钩子：按顺序执行
- `after_model` 钩子：按**反序**执行（...3, 2, 1）

这类似于洋葱模型，确保请求和响应经过相同的中间件层。

### Q2: 何时使用 Middleware vs Tools？

**A:**
- **Middleware**：用于横切关注点（日志、安全、上下文管理），影响所有工具调用
- **Tools**：用于特定的功能能力，代理可选择性调用

### Q3: Command 和 Conditional Edges 有什么区别？

**A:**
- **Command**：同时更新状态和路由（用于多代理交接）
- **Conditional Edges**：仅基于现有状态进行路由决策

### Q4: HITL 需要 Checkpointer 吗？

**A:** 是的，HITL 需要 checkpointer 来在中断期间持久化状态。可以使用：
- `InMemorySaver`（开发/测试）
- `PostgresSaver`（生产）
- 其他 BaseStore 实现

### Q5: 如何测试使用 Runtime Context 的工具？

**A:** 创建模拟的 Runtime 对象：

```python
from dataclasses import dataclass
from langchain.tools import ToolRuntime

@dataclass
class TestContext:
    user_id: str = "test_user"
    api_key: str = "test_key"

# Create mock runtime
mock_runtime = ToolRuntime(context=TestContext())

# Test tool
result = my_tool.invoke(
    {"query": "test"},
    runtime=mock_runtime
)
```

### Q6: 如何调试 Middleware？

**A:** 使用日志和状态检查：

```python
from langchain.agents.middleware import before_model, AgentState
from langgraph.runtime import Runtime

@before_model
def debug_middleware(state: AgentState, runtime: Runtime):
    print(f"State: {state}")
    print(f"Context: {runtime.context}")
    print(f"Messages: {len(state['messages'])}")
    return None

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[debug_middleware, ...],
)
```

### Q7: 如何优化多代理系统的性能？

**A:**
1. 使用更便宜的模型做路由决策
2. 缓存常见的路由决策
3. 减少不必要的代理切换
4. 并行执行独立的子代理
5. 使用流式输出提高响应速度

### Q8: Guardrails 会显著影响性能吗？

**A:** 取决于实现：
- **确定性 guardrails**（关键词过滤）：~1-5ms，影响很小
- **PII 检测**：~5-20ms，中等影响
- **基于模型的 guardrails**：~200-1000ms，较大影响

建议：将快速的确定性检查放在前面，昂贵的模型检查放在后面。

---

## 参考链接

### 官方文档
- [LangChain 官方文档](https://docs.langchain.com/)
- [LangGraph 文档](https://langchain-ai.github.io/langgraph/)
- [LangChain Python API Reference](https://reference.langchain.com/python/langchain/)
- [LangChain Middleware 文档](https://docs.langchain.com/oss/python/langchain/middleware/built-in)

### 博客和教程
- [Agent Middleware - LangChain Blog](https://blog.langchain.com/agent-middleware/)
- [LangChain 1.0 发布公告](https://blog.langchain.com/langchain-langgraph-1dot0/)
- [Middleware in LangChain 1.0 Alpha](https://changelog.langchain.com/announcements/middleware-in-langchain-1-0-alpha)

### GitHub 仓库
- [LangChain GitHub](https://github.com/langchain-ai/langchain)
- [LangGraph GitHub](https://github.com/langchain-ai/langgraph)
- [LangChain Templates](https://github.com/langchain-ai/langchain/tree/master/templates)

### 社区资源
- [LangChain Discord](https://discord.gg/langchain)
- [LangSmith Platform](https://smith.langchain.com/)
- [LangChain Twitter](https://twitter.com/langchainai)

---

## 总结

LangChain 的高级特性为构建生产级 AI 代理提供了强大的工具：

1. **Middleware** 提供了优雅的扩展机制，无需修改核心代理逻辑
2. **Multi-agent** 支持构建专业化、协作的代理系统
3. **Context Engineering** 优化提示和上下文管理，提高效率
4. **Human-in-the-Loop** 在关键决策点引入人工审核，确保可信度
5. **Guardrails** 提供多层安全保护，确保合规性和安全性
6. **Runtime** 通过依赖注入使代理更灵活、可测试和可配置

这些特性基于 LangGraph 的持久化运行时，提供了企业级的可靠性和可观测性。掌握这些高级特性，你将能够构建健壮、安全、高效的生产级 AI 代理系统。

**下一步学习建议**：
1. 实践每个特性的代码示例
2. 构建一个综合使用多个特性的完整项目
3. 阅读 LangSmith 文档学习监控和调试
4. 探索 LangGraph Cloud 了解部署选项
